---
title: "submission"
output: html_document
date: "2024-03-02"
---

```{r,warning=FALSE}
library(readr)
library(dplyr)
library(tidyverse)
library(MASS)
library(mixtools)
library(pwr)
library(mclust)
```

# Statistics in Action - Homework

**Group:** Max Rehman Linder, Patrick Tourniaire & Antonio Nisi

## 1. Single comparison

```{r}
# Import dataset

NHANES <- read_csv("NHANES_age_prediction 3.csv")
data = NHANES[,c("DIQ010","age_group","RIDAGEYR","RIAGENDR","PAQ605","BMXBMI","LBXGLU","LBXGLT","LBXIN")]
colnames(data) = c("Diabete","age_group","Age","Sex","Phys_activ","BMI","Glu","Glu2h","BIL")

rmarkdown::paged_table(data)
```

Feature descriptions for the above dataset:

-   `Diabete`: diabete diagnosis (1: Yes, 2: No, 3: Borderline)
-   `age_group` : age group of respondent ("Adult" or "Senior")
-   `Age` : the age of respondent
-   `Sex`: sex of respondent (1: Male, 2: Female)
-   `Physical activity`: respondent's answer to the question "Does your work involve vigorous-intensity activity that causes large increases in breathing or heart rate like carrying or lifting heavy loads, digging or construction work for at least 10 minutes continuously? (1: Yes, 2: No, 7: no answer)
-   `BMI` : Body Mass Index if respondent ($kg/m^{2}$)
-   `Glu` : Respondent's Blood Glucose after fasting ($mg/dL$)
-   `Glu2h` : Respondent's Two Hours Blood Glucose ($mg/dL$)
-   `BIL` : Respondent's Blood Insulin Level ($pmol/L$).

### 1.1 Mean level of blood glucose after fasting

**Task description:** Test if the mean level of blood glucose after fasting is the same for diabetic and non diabetic respondents (*hint:* plot first the data and justify the test(s) to use).

Before starting to compare the two groups of diabetics and non-diabetics we first split the dataset by filtering for the two cases.

```{r}
diabetic <- data %>% filter(Diabete == 1)
non_diabetic <- data %>% filter(Diabete == 2)

rmarkdown::paged_table(diabetic)
rmarkdown::paged_table(non_diabetic)

# Create a new combined table with only these two groups
diabetic$Group <- "Diabetic"
non_diabetic$Group <- "Non-Diabetic"

combined_data <- rbind(diabetic, non_diabetic)
combined_data
```

To be able to accurately test the mean blood glucose level for the two groups we have to plot the distribution of this data. This will allow us to identify the most applicable test as each hypothesis test have underlying assumptions which the data should be roughly following to achieve an accurate result through our tests.

To achieve this we create a box plot and a violin plot to understand the mean and variance of the data and to see how the data is distributed differently for the two groups through a violin plot.

```{r}
ggplot(combined_data, aes(x = Group, y = Glu)) +
  geom_boxplot() +
  labs(x = "Status", y = "Glucose") +
  ggtitle("Glucose Distribution by Diabetes Status") +
  theme_minimal()

ggplot(combined_data, aes(x = Group, y = Glu, fill = Group)) +
  geom_violin(trim = FALSE) +
  labs(x = "Status", y = "Glucose") +
  ggtitle("Glucose Distribution by Diabetes Status") +
  theme_minimal()
```

It is clear that the glucose distribution after fasting for the two groups have vastly different variances, thus a two-sample t-test would not be applicable due to its equal variance assumption. Further, it appears that the two distributions follow somewhat the parametric assumption. As a sanity check we can setup a test of the to distributions to verify that the variances should not be equal.

To achieve this we can perform an F-test which will test whether the variances of the two groups are equal. Thus, we will have the following hypotheses (where $d$ represents the diabetics and $\bar{d}$ represents the non-diabetics):

$$
\begin{align}
  H_0 &: \sigma_{d} = \sigma_{\bar{d}} \\
  H_1 &:\sigma_{d} \neq \sigma_{\bar{d}} \hspace{5mm} \text{for a two-tailed F-test}
\end{align}
$$

Since, we are only interested to see if the variances are unequal or not we use the two-tailed alternative hypothesis for the F-test.

```{r}
var.test(diabetic$Glu, non_diabetic$Glu, alternative = "two.sided")
```

Indeed, we can report a p-value of 2.2e-16 thus, we can reject the null hypothesis and conclude that the variances are indeed different. Which leads us to using the Welch student t-test which assumes parametric distribution with two groups of different variances. Again we use the same two sided alternative hypothesis but now for the means.

$$
\begin{align}
H_0 &: \mu_{d} = \mu_{\bar{d}} \\
H_1 &: \mu_{d} \neq \mu_{\bar{d}}
\end{align}
$$

With the following piece of code we can perform this Welch t-test on the means.

```{r}
t.test(diabetic$Glu, non_diabetic$Glu, alternative="two.sided")
```

Given that $\alpha = 0.05$ we can reject the null-hypothesis and say that our results of this test is significant and shows that we do not have enough evidence to conclude that the means between the two groups are indeed the same.

Furthermore, it is worth noting that the sample size between the two groups are different and thus the traditional t-test mechanisms are changed to account for this. However, the techniques used above will be able to handle this internally.

### 1.2 Diabetic mean blood glucose level between adults and seniors

**Task description:** Test for the diabetic respondents if the mean level of blood glucose after fasting is the same for adults and seniors.

To analyse what test will be the most applicable we will first plot the distributions, but to do that we will first need to create these two groups from our dataset.

```{r}
diabetic_adult <- data %>% filter(Diabete == 1 & age_group == "Adult")
diabetic_seniors <- data %>% filter(Diabete == 1 & age_group == "Senior")

rmarkdown::paged_table(diabetic_adult)
rmarkdown::paged_table(diabetic_seniors)

diabetic_adult$Group <- "Diabetic Adult"
diabetic_seniors$Group <- "Diabetic Senior"

combined_data <- rbind(diabetic_adult, diabetic_seniors)
rmarkdown::paged_table(combined_data)
```

We then use the new combined dataframe consisting only of these two groups to plot the distributions.

```{r}
ggplot(combined_data, aes(x = Group, y = Glu)) +
  geom_boxplot() +
  labs(x = "Age Group", y = "Glucose") +
  ggtitle("Glucose Distribution for Diabetics by Age Group") +
  theme_minimal()

ggplot(combined_data, aes(x = Group, y = Glu, fill = Group)) +
  geom_violin(trim = FALSE) +
  labs(x = "Age Group", y = "Glucose") +
  ggtitle("Glucose Distribution for Diabetics by Age Group") +
  theme_minimal()
```

From this we can observe that the distribution for the diabetic adults and diabetic seniors do not have the same variance and that they do not follow the parametric assumption. Which is clear by the fact that for seniors we have two distinct peaks which indicate that they do not follow the parametric distribution. Making the two sample Welch t-test not applicable in this scenario. Thus, to circumvent these limiting assumptions in this case we will be performing a Wilcoxon test which does not have an underlying parametric assumption about the distribution of the data.

Given the task description we can setup the following hypothesis test. Where $a$ represents the adult group and $s$ represents the senior group.

$$
\begin{align}
H_0 &: \mu_{a} = \mu_{s} \\
H_1 &: \mu_{a} \neq \mu_{s}
\end{align}
$$

Thus, we can perform the following Wilcox test to test this hypothesis.

```{r}
delta_wilcox <- wilcox.test(diabetic_seniors$Glu, diabetic_adult$Glu, alternative="two.sided")
delta_wilcox
```

Based on these results we can observe that we cannot reject the null-hypothesis, thus, we can say that there is not enough evidence to suggest that the mean blood glucose between diabetic adults and diabetic seniors is different.

### 1.3 Vigorous work activity mean blood glucose for adult diabetics

**Task description:** Is it possible to test for the adults diabetic respondents if the mean level of blood glucose after fasting is the same for those who have a vigorous work activity and for those who have not?

Again we will first filter the dataset and create a new combined dataset for these two groups.

```{r}
diabetic_active <- data %>% filter(Diabete == 1 & age_group == "Adult" & Phys_activ == 1)
diabetic_inactive <- data %>% filter(Diabete == 1 & age_group == "Adult" & Phys_activ == 2)

diabetic_active$Group <- "Diabetic Active"
diabetic_inactive$Group <- "Diabetic Inactive"

rmarkdown::paged_table(diabetic_active)
rmarkdown::paged_table(diabetic_inactive)

# Combine the data frames
combined_data <- rbind(diabetic_active, diabetic_inactive)
rmarkdown::paged_table(combined_data)

ggplot(combined_data, aes(x = Group, y = Glu)) +
  geom_boxplot() +
  labs(x = "Activity Group", y = "Glucose") +
  ggtitle("Glucose Distribution for Diabetics by Activity") +
  theme_minimal()

ggplot(combined_data, aes(x = Group, y = Glu, fill = Group)) +
  geom_violin(trim = FALSE) +
  labs(x = "Activity Group", y = "Glucose") +
  ggtitle("Glucose Distribution for Diabetics by Activity") +
  theme_minimal()
```

Immediately we can observe that we have significantly less data than for the other exercises, which can indicate that test results can be inconclusive. Furthermore, we cannot necessarily make an assumption about the normality of the data due to the fact that we have such little samples. It could for example be the case that for many more samples that the distribution looks much more like a normal.

However, for now we will not make any normality assumption and we will be using a Wilcoxon test where we will analyse its power of varying number of samples for $\alpha=0.05$ which will enable us to see how the power of the test evolves over the number of samples.

```{r}
delta_wilcox  <- wilcox.test(diabetic_active$Glu, diabetic_inactive$Glu, alternative="two.sided")$statistic
alpha <- 0.05
seq_n <- seq(10,1)
power_wilcox  <- map_dbl(seq_n, ~power.t.test(n = .x, delta_wilcox , sig.level = alpha)$power)
data.frame(power = c(power_wilcox), n = rep(seq_n),
           method = rep(c("Wilcox"), each = length(seq_n))) %>% 
  ggplot() + aes(x = n, y = power, color = method) + geom_line()
```

We observe that for a sample size of 4, the power of the Wilcoxon t-test is more or less 1.0 meaning that it is almost guaranteed that the null-hypothesis will be rejected when it is false. Furthermore, we plot for $n=4$ the different values of $\alpha$ wrt. the power of the Wilcoxon test.

```{r}
delta_wilcox  <- wilcox.test(diabetic_active$Glu, diabetic_inactive$Glu, alternative="two.sided")$statistic
seq_alpha <- 10^seq(-12, -5, .1)
n <- 4
power_wilcox  <- map_dbl(seq_alpha, ~power.t.test(n, delta_wilcox , sig.level = .x)$power)
data.frame(power = c(power_wilcox), alpha = rep(seq_alpha, 1),
           method = rep(c("Wilcox"), each = length(seq_alpha))) %>% 
  ggplot() + aes(x = alpha, y = power, color = method) + geom_line()
```

As observed earlier for a $\alpha=0.05$ we basically have guaranteed that the null-hypothesis is rejected correctly only after 2 samples. However, this seems strange and thus one might want to adjust the $\alpha$ value to get a more reasonable sample requirement.

### 1.4 Same proportion of male and female diabetics

**Task description:** Test if the proportion of diabetic is the same for male and female respondents. Compare conclusions of several tests. You can use the function `table` to compute contingency tables.

```{r}
diabete_and_nondiabete = filter(data, Diabete != 3)
contingency_table <- table(diabete_and_nondiabete$Sex, diabete_and_nondiabete$Diabete)
contingency_table
```

```{r}
fisher.test(contingency_table)
```

```{r}
diabetes = c(8, 13)
population = c(nrow(filter(data, Sex==1)), nrow(filter(data, Sex==2)))

prop.test(diabetes, population, correct = FALSE)
```

```{r}
prop.test(diabetes, population, correct = TRUE)
```

## 2. Gene expression data

```{r}
load("liver_data.rda")
```

## 2.1 Finding most significant discovery

**Task description:**: We first want to study the gene which is the most correlated with the level of cholesterol. Identify this gene and plot cholesterol as a function of this gene expression level. Fit a linear model. Test if there is a significant relation between this gene expression and cholesterol?

We first want to study the gene which is the most correlated with the level of cholesterol. Identify this gene and plot `cholesterol` as a function of this gene expression level. Fit a linear model. Test if there is a significant relation between this gene expression and `cholesterol`?

```{r}
labels <- colnames(liver)

maxSlope=0
minP=1

for (i in 2:3117){
  lmgene <- lm(cholesterol~liver[,i], data=liver)
  slope <- abs(coef(lmgene, complete = TRUE)[2])
  summary_stats <- summary(lmgene)
  p_value <- summary_stats$coefficients[2, 4]
  
  if (slope > maxSlope){
    maxSlope=slope
    max_slope_model=lmgene
  }
  
  if (minP > p_value){
    minP=p_value
    min_p_model=lmgene
    genename=labels[i]
  }
}

```

Fitting a linear model for every gene, we get 3116 different models in total. The null-hypothesis is that a gene does no have a correlation with the cholesterol-level, in other words, the slope is zero in this case. Consequently, the gene corresponding to the model with the lowest p-value is also the model with highest slope. In our case, that turned out to be gene $A\_42P631473$ with a p-value of less than $10^{-13}$.

```{r}

ggplotRegression <- function (fit) {

require(ggplot2)

ggplot(fit$model, aes_string(x = names(fit$model)[2], y = names(fit$model)[1])) + 
  geom_point() +
  stat_smooth(method = "lm", col = "red") +
  labs(title = paste("Adj R2 = ",signif(summary(fit)$adj.r.squared, 5),
                     "Intercept =",signif(fit$coef[[1]],5 ),
                     " Slope =",signif(fit$coef[[2]], 5),
                     " P =",signif(summary(fit)$coef[2,4], 5)), x = paste(genename, "-expression"))
}

ggplotRegression(min_p_model)
#ggplotRegression(max_slope_model)


```

## 2.2 Calculating all p-values
**Task description:**: We now wish to perform the previous test for all of the 3116 genes. For each of the genes, fit a linear model that explains cholesterol as a function of the gene expression and compute the p-value of the test.

We now do the same test for all 3116 genes and fit a linear model that explains `cholesterol` as a function of the gene expression. Doing so gives us a relatively large proportion of genes that correspond to a p-value of less than 0.05

```{r}
labels <- colnames(liver)

p_list <- c()
lineList = c()

for (i in 2:3117){
  lmgene <- lm(cholesterol~liver[,i], data=liver)
  slope <- abs(coef(lmgene, complete = TRUE)[2])
  summary_stats <- summary(lmgene)
  p_value <- summary_stats$coefficients[2, 4]
  p_list=c(p_list, p_value)
  lineList = c(lineList, (i-1)/3116)
}

```

```{r}
plot(hist(p_list, breaks=20, col = c("orange", rep("lightblue", 100))), main = "Density of p-values", xlab = "p-values")

```

## 2.3 Ordering p-values

**Task description:**Order the p-values and plot the ordered as a function of their rank. On the same plot, display the line y=x/3116. Discuss the result.

Plotting all values, we can conclude that there are amount of models that have a p-value below 0.05. It is tempting to make the conclusion that all these models are statistically significant since they all have a small p-value. However, in the event that the we only have random data, we still expect 5% of the models to appear significantly significant. In fact, if we were to only have random data, we expect a uniform distribution between 0 and 1 for the p-values. Visually, looking at the figure above, we can the p-values perform better that the uniform distribution and therefore appear to contain significant discoveries.

```{r}

p_list=sort(p_list)
plot(p_list, col = "blue", lwd = 2, xlab = "Rank", ylab = "p-value")
lines(lineList, col = "red", lwd = 2)

legend("topleft", legend = c("p-values", "Uniform line"), col = c("blue", "red"), lwd = 2)


```

Shuffling all labels, we get an approximate uniform distribution of p-values which corresponds to the p-values following a straight line

```{r}
labels <- colnames(liver)

p_list_shuffeled <- c()
lineList_shuffeled = c()

for (i in 2:3117){
  cholesterol=liver$cholesterol
  randomLabel <- sample(cholesterol)
  lmgene <- lm(randomLabel~liver[,i], data=liver)
  slope <- abs(coef(lmgene, complete = TRUE)[2])
  summary_stats <- summary(lmgene)
  p_value <- summary_stats$coefficients[2, 4]
  p_list_shuffeled=c(p_list_shuffeled, p_value)
  lineList_shuffeled = c(lineList_shuffeled, (i-1)/3116)
}

p_list_shuffeled=sort(p_list_shuffeled)
plot(p_list_shuffeled, col = "blue", lwd = 2, xlab = "Rank", ylab = "p-value", main = "Shuffeled labels")
lines(lineList_shuffeled, col = "red", lwd = 2)

legend("topleft", legend = c("p-values when shuffeled labels", "Uniform line"), col = c("blue", "red"), lwd = 2)

plot(hist(p_list_shuffeled, breaks=20, col = c("orange", rep("lightblue", 100))), main = "Density of p-values", xlab = "p-values")

```
## 2.4 Benjamini-Hochberg procedure
**Task description:**Identify a set of genes linked to the response (aka discoveries). We want to guarantee that the expected proportion of false discoveries (mistakes) is less than 5%. Explain how you proceed and how many genes you discover.

In the nest question, we are tasked with finding the maximum number of genes such that the expected number of false discoveries is at most 5%. For this method, we use the Benjamini-Hochberg procedure \cite{FDR}. In short, it states that if we have a set of $\{ H_1, H_2, ... \: , H_m \}$ with associated and ordered p-values $\{ p_1, p_2, ... \: , p_m \}$, then the threshold for the index can be written as:

$$ P_{BH}= max_i \left\{ P_{(i)} : P_{(i)} \leq \alpha \frac{i}{m}      \right\} $$



```{r}

soft_guarantee=c()
alpha=0.05
index=0
n=3116



for (i in 1:3116){
  val=i*alpha/n
  soft_guarantee=c(soft_guarantee, val)
  if (p_list[i]<val){
    index=i
  }

}



plot(p_list, col = "blue", lwd = 2, xlab = "Index", ylab = "p-value")
lines(soft_guarantee, col = "red", lwd = 2)
abline(v=index, col="black")
legend("topleft", legend = c("Benjamini-Hochberg boundary", "P-value", paste("Intersect=", as.character(index))), col = c("red", "blue", "black"), lwd = 2)

plot(p_list[1:1200], col = "blue", lwd = 2, xlab = "Index", ylab = "p-value")
lines(soft_guarantee[1:1200], col = "red", lwd = 2)
abline(v=index, col="black")
legend("topleft", legend = c("Benjamini-Hochberg boundary", "P-value", paste("Intersect=", as.character(index))), col = c("red", "blue", "black"), lwd = 2)


```

We can see that $\alpha \frac{i}{m}$ plots out a line, and we choose i such that $P_{(i)}$ always is smaller or equal to that value. Doing so, we get 941 discoveries such that the expected number of false discoveries is no more than 5 %

##2.5 Holm–Bonferroni method
**Task description:** We wish to be more conservative and guarantee that the probability of making a false discovery (or more) is less than 5%. Explain how you proceed and how many genes you discover.


A stronger guarantee would be that on impose that there shouldn't be more than a 5 % risk that any discovery is false. For this method, we use the Holm–Bonferroni method. The method is as follows:

I have my ordered p-values, $\{ P_1, P_2, ... \: , P_m \}$, and want the FWER to be no higher than a level $\alpha$.

1.  Is $P_1 < \alpha/m$? Reject $H_1$ and continue to the next step, otherwise EXIT.

2.  Is $P_2 < \alpha/(m-1)$? so, reject $H_2$ and continue to the next step, otherwise EXIT.

3.  Continue this iterative process as long as $P_k < \frac{\alpha}{m+1-k}$.

    Using this process, we get the following result:

```{r}

strong_guarantee=c()
alpha=0.05
index=0

for (i in 1:n){
  val=alpha/(n+1-i)
  strong_guarantee=c(strong_guarantee, val)
  if (p_list[i]<val){
    index=i
  }

}


plot(p_list, col = "blue", lwd = 2, xlab = "Index", ylab = "p-value")
lines(strong_guarantee, col = "red", lwd = 2)
abline(v=index, col="black", )
legend("topleft", legend = c("Holm–Bonferroni", "P-value", paste("Intersect=", as.character(index))), col = c("red", "blue", "black"), lwd = 2)


plot(p_list[1:225], col = "blue", lwd = 2, xlab = "Index", ylab = "p-value")
lines(strong_guarantee, col = "red", lwd = 2)
abline(v=index, col="black", )
legend("bottomright", legend = c("Holm–Bonferroni", "P-value", paste("Intersect=", as.character(index))), col = c("red", "blue", "black"), lwd = 2)



```

Using this process, we see that we get 223 discoveries. We also not that smallest p-value in that set is approximately equal to $1.7 \cdot 10^{-5}$.


## 3. Non parametric regression

## 4. S&P500 daily return

### 4.1 Data loading and computing daily return

**Task description:** Load the dataset `sp500_history.csv` into a *data frame* and add a column that computes the daily return for each days.

```{r}
sp500_data <- read_csv("sp500_history.csv")
sp500_data$DailyReturn <- sp500_data$Close - sp500_data$Open

# Check for non-numeric values in DailyReturn column
non_numeric_count <- sum(is.na(sp500_data$DailyReturn) | !is.numeric(sp500_data$DailyReturn))
cat("Number of non-numeric values in DailyReturn column:", non_numeric_count, "\n")

if (non_numeric_count > 0) {
  cat("Indices of non-numeric values in DailyReturn column:", which(!is.numeric(sp500_data$DailyReturn)), "\n")
} else {
  cat("No non-numeric values found in DailyReturn column.\n")
}

rmarkdown::paged_table(sp500_data)
```

### 4.2 Daily return sampled from a normal distribution

**Task description:** We propose to model the daily return as a sample from a normal population. Write the model and use *R* to fit it to the data. What do you think of this model?

To first understand the data, we plot the daily return as a function of the day number.

```{r}
ggplot(sp500_data, aes(x = Date, y = DailyReturn)) +
geom_histogram(stat = "identity", fill = "grey", color = "grey", bins = 15) +
labs(title = "Daily Return of S&P 500 (2007-2010)",
     x = "Date",
     y = "Daily Return") +
theme_minimal() +
theme(axis.text.x = element_blank(),
      axis.title.x = element_blank())
```

Under Louis Bachelier's "Theory of Speculation" assume that the returns can be sampled from a perfectly random distribution, in this case this would be a normal distribution which we can parametirise through a Gaussian model. Assume that the return is modeled by some random variable $X_r$ and that some value of that RV is denoted by $x_t$. Thus, since we first assume Bachelier's theory we can model the RV through the following Gaussian.

$$    X_r \sim \mathcal{N}(\mu_r, \sigma_r^2)    $$ Where the model parameters are the empirical mean and empirical variance expressed by: $$    \mu_r = \dfrac{\sum_{i=0}^N x_i}{N} \hspace{5mm} \sigma_r^2 = \dfrac{\sum_{i=0}^N (x_i - \mu_r)}{N}    $$ We can then directly fit such a Gaussian model in R through the following:

```{r}
fit <- fitdistr(sp500_data$DailyReturn, "normal")
fit

# Plot the histogram of daily returns with the fitted Gaussian distribution
hist(sp500_data$DailyReturn, breaks = 100, freq = FALSE, 
     main = "Daily Returns with Fitted Gaussian Distribution",
     xlab = "Daily Return")
curve(dnorm(x, mean = fit$estimate[1], sd = fit$estimate[2]), 
      col = "blue", lwd = 2, add = TRUE, yaxt = "n")
```

Indeed, the it seems that Bachelier's theory does describe the price movements well. As the daily return resembles a normal distribution, however, we do observe that that tails seem to follow a higher variance compared to the density around the mean which seems to have a smaller variance. Which also explains why our initial Gaussian doesn't effectively capture this density. Thus, we can motivate why a Gaussian mixture model might perform better in this scenario.

### 4.3 Gaussian Mixture Model

**Task description:** Instead of a single normal, we now propose to use a mixture of normals with $p$ components. Write the corresponding models and use *R* to fit them to the data for all values of $p$ between $2$ and $6$. Discuss the results and compare them with the model of Question 1.

To combine the multiple Gaussians to model a single valid PDF, we will have to introduce a weight parameter which will be used between the weighted sum of Gaussian components. First, we denote a single component the following way. $$    Y_r \sim \mathcal{N}(\theta_k) \hspace{5mm} \text{Where $\theta$ is a parameter vector for component $k$ defined by} \hspace{5mm} \theta_k = \left[ \mu_k, \sigma_k^2 \right]    $$ Furthermore, to form the mixture we introduce the weight parameter $\pi_k$ which is probabilistic and forms the weighted sum (GMM). $$    Y_i = \sum_{k=1}^K \pi_k \mathcal{N}(\theta_k)    $$ As this is a weighted sum and we expect $Y_i$ to be modeled from a valid PDF, we have to constrain the weights such that: $$    \pi_k \geq 0 \hspace{2.5mm} \land \hspace{2.5mm} \sum_{k=1}^K \pi_k = 1    $$ Which is the case as long as we follow that $\pi_k$ comes from a PDF defined over the categorical RVs $Z_i$ which defines the different components such that we have: $$    P(Z_i = k) = \pi_k \hspace{5mm} \text{Where $Z$ is a sequence of RVs} \hspace{5mm} \{1, 2, \dots, K\}    $$ Thus, we observe that we first have to initialize the parameters of the model which we can do by placing the means based on a K-Means initialization and set some explicit variance which will serve as the initialization for the iterative EM-algorithm for parameter optimization over such a GMM. First off, we will naively apply a GMM routine to this problem using expectation maximization which we will compare against an constrained optimization approach where we ensure equal means for all distributions as it seems to only be the variance that's needed to change based on our earlier graph.

#### 4.3.1 Unconstrained EM Optimization of GMMs

For this set of experiments we will be using the general GMM approach outlines above, where we initialize the means using k-Means and perform EM to optimize the model parameters.

```{r}
n_components = 3
y = sp500_data$DailyReturn

kmeans_out <- kmeans(y, centers = n_components)
mu = as.vector(kmeans_out$centers)
mu
```

This results seems strange given that we should want Gaussian components around the same mean. However, for this experiment we will naively apply k-Means initialization without any constraints in the optimization and observe what results we obtain.

```{r}
# Function which fits n gaussian components and plots the components with the final mixture density.
fit_mixture_model <- function(n_components, y, max_iter = 400) {
  kmeans_out <- kmeans(y, centers = n_components)
  mu <- as.vector(kmeans_out$centers)
  mixture_EM <- normalmixEM(y, mu = mu, k = n_components, arbvar = FALSE, maxit = max_iter)
  log_likelihood <- mixture_EM$loglik
  
  result <- list(pi = mixture_EM$lambda, mu = mixture_EM$mu, sigma = mixture_EM$sigma)
  
  # Plot histogram with 50 bars
  hist(y, breaks = 50, probability = TRUE, col = alpha("skyblue", 0.25), border = alpha("black", 0.25), main = paste("Mixture Model with", n_components, "Components"), xlab = "Daily Return", ylab = "Density")
  
  # Add density line
  final_density <- rowSums(sapply(1:length(mixture_EM$lambda), function(i) mixture_EM$lambda[i] * dnorm(x, mean = mixture_EM$mu[i], sd = mixture_EM$sigma[i])))
  
  x <- seq(min(y), max(y), length.out = 1000)
  lines(x, final_density, lwd = 2)
  
  # Plot densities of Gaussian components
  for (i in 1:n_components) {
    curve(result$pi[i] * dnorm(x, mean = result$mu[i], sd = result$sigma[i]), add = TRUE, col = i + 1, lwd=2, lty=2)
  }
  
  return(list(result = result, mixture_EM = mixture_EM, log_likelihood = log_likelihood))
}
```

Using the above function, we can plot the resulting fits for $2$ to $6$ Gaussian components.

```{r}
mx2 <- fit_mixture_model(2, y)
mx3 <- fit_mixture_model(3, y)
mx4 <- fit_mixture_model(4, y)
mx5 <- fit_mixture_model(5, y)
mx6 <- fit_mixture_model(6, y)

mixtures = c(mx2, mx3, mx4, mx5, mx6)
```

We can observe that the final fits are indeed much better than the single Gaussian example. However, we do observe that there is some overfitting happening at the tails of the distribution for all of the experiments we have run. This is caused by the fact that the means are not the same for all the Gaussian components and thus for our next section we will be looking at constrained optimization of this GMM where the means should be the same.

#### 4.3.2 Constrained EM Optimisation of GMMs

To be able to optimize a GMM where the component means are constrained to be equal, we have to modify the classic expectation maximization algorithm to account for this specific constraint. First off, lets define the basic function for computing the density of a GMM which we will use later.

```{r}
dcomponents <- function(theta, x) {
    mapply(
      function(pi, mu, sigma) pi * dnorm(x, mu, sigma),
      theta$pi, theta$mu, theta$sigma,
      SIMPLIFY = TRUE
    )
}
```

Now that we have the above definition of a GMM density we can define how our new EM algorithm is going to work. As the EM algorithm is broken into two steps (E-step and M-step), we will first define the E-step.

**E-step**

As presented earlier we assume that we have some labels $Z_i$ which indicate what components model what sub-parts of the data. However, this is not something that we can know from beforehand. Thus, we compute the expected value of the statistic $\mathbb{E}[S(Z, Y) | Y ; \theta^{(t-1)}]$. Which we can denote using $\tau_{ik}^t$.

$$
\begin{align}
\tau_{ik}^t &= \mathbb{E}[S(Z,Y) | Y; \theta^{(t-1)}] \\
            &= \dfrac{\pi_k^{(t-1)} f_k(y_i;\theta^{(t-1)})}{\sum_{\mathcal{l}=0}^K \pi_k^{(t-1)} f_{\mathcal{l}} (y_i ; \theta^{(t-1)})}
\end{align}
$$

Indeed, we see that there is no need to change anything in our E-step to achieve this constraint. However, we will have to deal with this in the M-step.

**M-step**

For the M-step we follow a similar procedure but now we have to change the update for the means. Which will take the following form. First we have the same parameters,

$$
\begin{align}
\hat{\pi}_k = \dfrac{\sum_{i=1}^K \tau_{ik}}{n} \hspace{5mm} \mu_k = \dfrac{\sum_{i=1}^K \tau_{ik} x_i}{\sum_{i=1}^K \tau_{ik}} \hspace{5mm} \hat{\sigma}_k^2 = \dfrac{\sum_{i=1}^K \tau_{ik} z_i^2}{\sum_{i=1}^K \tau_{ik}} - \mu_k^2
\end{align}
$$

Note that we do not denote the mean with $\hat{\mu}$ above as it is only used on the optimization for the variance, to compute the new $\hat{\mu}$ we do the following.

$$
\hat{\mu}_k = \frac{\sum_{i=1}^K w_k \mu_k}{\sum_{i=1}^K w_k}
$$

Where $w_k$ is computed by.

$$
w_k = \dfrac{\sum_{i=1}^K \tau_{ik}}{\hat{\sigma}_{k}^2}
$$

We can then use this to create a modified version of the EM algorithm to deterministically obtain components with equal means.

```{r}
# M-step functions, we include the general case such that we can verify the correctness of our implementation.

M_step_general <- function(tau, x) {
  pi    <- colMeans(tau)
  mu    <- colSums(tau * x) / colSums(tau)
  sigma <- sqrt(colSums(tau * x^2) / colSums(tau) - mu^2)
  list(pi = pi, mu = mu, sigma = sigma)
}

M_step_same_means <- function(tau, x) {
  n <- length(x)
  K <- ncol(tau)
  pi    <- colSums(tau) / n
  muk   <- colSums(tau * x) / colSums(tau)
  sigma <- sqrt(colSums(tau * x^2) / colSums(tau) - muk^2)
  wk    <- n * pi / sigma^2
  mu    <- rep(sum(wk * muk) / sum(wk), K)
  list(pi = pi, mu = mu, sigma = sigma)
}
```

```{r}
mixture_gaussian1D <- function(x, theta0, M_step = M_step_general, max_iter = 100, threshold = 1e-6) {

  ## initialization
  n <- length(x)
  likelihoods  <- dcomponents(theta0, x) 
  deviance     <- numeric(max_iter)
  deviance[1]  <- -2 * sum(log(rowSums(likelihoods)))

  for (t in 1:max_iter) {
    
    # E step
    tau <- likelihoods / rowSums(likelihoods)
    # M step
    theta <- M_step(tau, x)
    
    ## Assessing convergence
    likelihoods   <- dcomponents(theta, x)
    deviance[t+1] <- - 2 * sum(log(rowSums(likelihoods)))

    ## prepare next iterations
    
    if (abs(deviance[t + 1] - deviance[t]) < threshold)
      break
    
  }


  list(theta = theta, deviance = deviance[t + 1])
}
```

```{r}
mu_data = mean(y)
pi = c(.5, .5)
means = c(mu_data, mu_data)
variances = c(10,2)

theta0  <- list(pi=pi, mu=means, sigma=variances)
out_general <- mixture_gaussian1D(sp500_data$DailyReturn, theta0, M_step = M_step_same_means)
out_general$theta
```

To then compare the results of using different number of components we additionally define a experiment function.

```{r}
softmax <- function(x) {
  exp_x <- exp(x)
  exp_x / sum(exp_x)
}

constrained_gmm_experiment <- function(y, variances) {
  n_components = length(variances)
  
  mu_data = mean(y)
  
  pi = softmax(rep(1, n_components))
  means = rep(mu_data, n_components)
  
  theta0  <- list(pi=pi, mu=means, sigma=variances)
  out_general <- mixture_gaussian1D(sp500_data$DailyReturn, theta0, M_step = M_step_same_means)
  out_general$theta
  
  # Define the data
  x <- seq(min(y), max(y), length.out = 1000)
  
  # Compute the densities for the two components
  densities <- dcomponents(out_general$theta, x)
  
  # Calculate the final density of the mixture
  final_density <- rowSums(sapply(1:length(out_general$theta$pi), function(i) out_general$theta$pi[i] * dnorm(x, mean = out_general$theta$mu[i], sd = out_general$theta$sigma[i])))
  
  # Plot the data points
  hist(y, breaks = 50, probability = TRUE, col = alpha("skyblue", 0.25), border = alpha("black", 0.25), main = paste("Mixture Model with", n_components, "Components"), xlab = "Daily Return", ylab = "Density")
  
  # Plot the densities for each component
  for (i in 1:n_components) {
    lines(x, densities[,i], col = rainbow(n_components)[i], lwd=2, lty=2)
  }
  
  # Plot the final density of the mixture
  lines(x, final_density, lwd = 2)
  
  # Add legend
  legend_labels <- c(paste0("Component ", 1:n_components), "Mixture")
  legend("topright", legend = legend_labels, col = c(rainbow(n_components), "black"), lty = 1, lwd = 2, pch = 20)
}
```

```{r}
constrained_gmm_experiment(y, variances = c(10, 7))
constrained_gmm_experiment(y, variances = c(10, 7, 5))
constrained_gmm_experiment(y, variances = c(10, 7, 5, 3))
constrained_gmm_experiment(y, variances = c(10, 7, 5, 3, 1))
constrained_gmm_experiment(y, variances = c(12, 10, 7, 5, 3, 1))
```

### 4.4 Student t-distribution

**Task description:** As a third and last model, we propose to model the daily return as coming from a location-scale family of Student distributions, which is a model defined by the collection of densities $$f_{\nu,m,a}(x) = \frac{\Gamma\big(\frac{\nu+1}{2}\big)}{\sqrt{\pi \nu a^2}\Gamma\big(\frac{\nu}{2}\big)}\Bigg(1+\frac{(x-m)^2}{\nu a^2} \Bigg)^{-\frac{\nu+1}{2}}$$ where $\Gamma$ is the Gamma function and $\theta=(\nu,m,a) \in \mathbb{R}_+^*\times \mathbb{R}\times \mathbb{R}_+^*$ is the parameter.

#### 4.4.1 Motivation behind model selection

**Task description:** What is the motivation for considering this model?

Indeed from looking at the above examples the final fits are still not great even when we add additional components to account for the variable variance of the distribution. But as we observe the distribution it is more clear that it can follow more closely a student t-distribution given its long tails and elongated middle density.

#### 4.4.2 Fitting the student t-distribution

**Task description:** Propose an algorithm for estimating the parameter of this model and use it to fit this model to the data.

```{r}
# Define log-likelihood function for Student's t-distribution
neglog_likelihood_student_t <- function(theta, x) {
  nu <- theta[1]
  mu <- theta[2]
  sigma <- theta[3]
  
  # Check if sigma is positive
  if(sigma <= 0) {
    return(Inf)
  }
  
  # Check if nu is at least 1
  if(nu < 1) {
    nu <- 1
  }
  
  loglik <- sum(dt((x - mu) / sigma, df = nu, log = TRUE)) - length(x) * log(sigma)
  
  return(-loglik)  # Negative log-likelihood for minimization
}

# Fit Student's t-distribution model using MLE
fit_student_t <- function(y) {
  if (any(!is.finite(y))) {
    stop("Input data contains non-finite values.")
  }
  
  y <- y[is.finite(y)]
  
  if (length(y) < 3) {
    stop("Insufficient data points for estimation.")
  }
  
  init_params <- c(5, mean(y), sd(y))
  opt_result <- optim(init_params, neglog_likelihood_student_t, x = y, method = "L-BFGS-B",
                      lower = c(0, -Inf, 0), upper = c(Inf, Inf, Inf))
  
  # Extract optimized parameters
  nu <- opt_result$par[1]
  mu <- opt_result$par[2]
  sigma <- opt_result$par[3]
  
  # Return parameter estimates
  return(list(nu = nu, mu = mu, sigma = sigma))
}

# Fit Student's t-distribution model to the data
student_t_params <- fit_student_t(y)
student_t_params
```

```{r}
# Generate data for the PDF using the fitted parameters
x_range <- seq(min(y), max(y), length.out = 1000)
pdf_values <- dt((x_range - student_t_params$mu) / student_t_params$sigma, 
                 df = student_t_params$nu) / student_t_params$sigma

# Create a data frame for plotting
plot_data <- data.frame(x = x_range, pdf = pdf_values)

# Plot histogram of the data and fitted PDF
ggplot() +
  geom_histogram(data = data.frame(y), aes(x = y, y = after_stat(density)), bins = 30, fill = "skyblue", color = "black") +
  geom_line(data = plot_data, aes(x = x, y = pdf), color = "red", size = 1) +
  labs(title = "Fitted Student's t-distribution Model",
       x = "Daily Return",
       y = "Density") +
  theme_minimal()
```

### 4.5 Model evaluation

**Task description:** Between all the previous models (the normal, the 5 mixtures of normals, and the location-scale Student) which one do you choose? Explain your methodology.

To effectively evaluate the models we can employ a few techniques, the most obvious is to apply the BIC metric to measure how well the models capture the distribution of the data. Which is simply done by applying the following formula:

$$
BIC = k \hspace{1mm} ln(n) - 2 \hspace{1mm} ln(\hat{L})
$$

Where we have the following parameters to compute:

-   $\hat{L}$: the maximized likelihood of the model.

-   $n$: the number of data points in the dataset.

-   $k$: the number of free parameters in our model (note for the constrained GMM this will be different).

We then go ahead and apply this metric to the models so far and put the results in a table seen below.

```{r}
bic_benchmark <- function(x, out_model, n_free_params) {
  n <- length(x)
  bic_values = list()
  
  return(out_model$deviance + log(n) * n_free_params)
}
```

```{r}
mu_data = mean(y)
pi = c(.2, .2, .2, .2, .2)
means = c(mu_data, mu_data, mu_data, mu_data, mu_data)
variances = c(10, 7, 5, 3, 1)

theta0  <- list(pi=pi, mu=means, sigma=variances)
out_general <- mixture_gaussian1D(sp500_data$DailyReturn, theta0, M_step = M_step_general)

b_5unconstrained <- bic_benchmark(y, out_general, 3 * 5)

out_general_constrained <- mixture_gaussian1D(sp500_data$DailyReturn, theta0, M_step = M_step_same_means)

b_5constrained <- bic_benchmark(y, out_general_constrained, 2 * 5 + 1) # Only one mu is optimised

theta = student_t_params
b_tdist = 3 * log(length(y)) + 2 * neglog_likelihood_student_t(c(theta$nu, theta$mu, theta$sigma), y)

single_guass <- fitdistr(sp500_data$DailyReturn, "normal")
b_single_gauss <- BIC(single_guass)
```

```{r}
rbind(
  five_comp_gmm             = b_5unconstrained,
  five_comp_gmm_constrained = b_5constrained,
  student_tdistribution     = b_tdist,
  single_gaussian           = b_single_gauss
) %>% as.data.frame() %>% setNames("BIC") %>% rmarkdown::paged_table()
```

By only observing the BIC values it might seem that the single Gaussian is the best model. However, this should not be the case, indeed we need to perform additional tests. The Kolmogorov-Smirnov test can work for this, and we can expect to see that the single Gaussian is rejected. Where we have the following null- and alternative hypothesis

$$
H_0: \text{the empirical distribution of the data fits the theoretical distribution} \\
H_1: \text{The empirical distribution does not fit the theoretical distribution}
$$

```{r}
ks.test(y, "pmixture", out_general_constrained$theta)
```

```{r}
ks.test(y, "pmixture", out_general$theta)
```

```{r}
ks.test(y, "pmixture", theta0)
```

```{r}
# Function to compute CDF of Student's t-distribution
cdf_student_t <- function(x, nu, mu, sigma) {
  pt((x - mu) / sigma, df = nu)
}

# Perform KS test
ks_test_student_t <- function(y, student_t_params) {
  # Extract parameters
  nu <- student_t_params$nu
  mu <- student_t_params$mu
  sigma <- student_t_params$sigma
  
  # Compute CDF values
  cdf_values <- cdf_student_t(sort(y), nu, mu, sigma)
  
  # Compute ECDF values
  ecdf_values <- ecdf(y)(sort(y))
  
  # Perform KS test
  ks_result <- ks.test(ecdf_values, cdf_values)
  
  return(ks_result)
}

# Perform KS test on the data using the fitted Student's t-distribution parameters
ks_result <- ks_test_student_t(y, student_t_params)
ks_result

```

Thus, we can quite confidently conclude that we can reject the single Gaussian as a model (as the null-hypothesis is rejected) and announce the Student t-distribution to be the best distribution given the data. Which makes intuitive sense given the fact that the distribution of the data much more resemble that of a Student t-distribution.

### 4.6 Expected daily return

**Task description:** Is the expected daily return different than zero?
