---
title: "submission"
output: html_document
date: "2024-03-02"
---

```{r,warning=FALSE}
library(readr)
library(dplyr)
library(tidyverse)
library(MASS)
library(mixtools)
library(pwr)
```

# Statistics in Action - Homework

**Group:** Max Rehman Linder, Patrick Tourniaire & Antonio Nisi

## 1. Single comparison

```{r}
# Import dataset

NHANES <- read_csv("NHANES_age_prediction 3.csv")
data = NHANES[,c("DIQ010","age_group","RIDAGEYR","RIAGENDR","PAQ605","BMXBMI","LBXGLU","LBXGLT","LBXIN")]
colnames(data) = c("Diabete","age_group","Age","Sex","Phys_activ","BMI","Glu","Glu2h","BIL")

rmarkdown::paged_table(data)
```

Feature descriptions for the above dataset:

-   `Diabete`: diabete diagnosis (1: Yes, 2: No, 3: Borderline)
-   `age_group` : age group of respondent ("Adult" or "Senior")
-   `Age` : the age of respondent
-   `Sex`: sex of respondent (1: Male, 2: Female)
-   `Physical activity`: respondent's answer to the question "Does your work involve vigorous-intensity activity that causes large increases in breathing or heart rate like carrying or lifting heavy loads, digging or construction work for at least 10 minutes continuously? (1: Yes, 2: No, 7: no answer)
-   `BMI` : Body Mass Index if respondent ($kg/m^{2}$)
-   `Glu` : Respondent's Blood Glucose after fasting ($mg/dL$)
-   `Glu2h` : Respondent's Two Hours Blood Glucose ($mg/dL$)
-   `BIL` : Respondent's Blood Insulin Level ($pmol/L$).

### 1.1 Mean level of blood glucose after fasting

**Task description:** Test if the mean level of blood glucose after fasting is the same for diabetic and non diabetic respondents (*hint:* plot first the data and justify the test(s) to use).

Before starting to compare the two groups of diabetics and non-diabetics we first split the dataset by filtering for the two cases.

```{r}
diabetic <- data %>% filter(Diabete == 1)
non_diabetic <- data %>% filter(Diabete == 2)

rmarkdown::paged_table(diabetic)
rmarkdown::paged_table(non_diabetic)

# Create a new combined table with only these two groups
diabetic$Group <- "Diabetic"
non_diabetic$Group <- "Non-Diabetic"

combined_data <- rbind(diabetic, non_diabetic)
combined_data
```

To be able to accurately test the mean blood glucose level for the two groups we have to plot the distribution of this data. This will allow us to identify the most applicable test as each hypothesis test have underlying assumptions which the data should be roughly following to achieve an accurate result through our tests.

To achieve this we create a box plot and a violin plot to understand the mean and variance of the data and to see how the data is distributed differently for the two groups through a violin plot.

```{r}
ggplot(combined_data, aes(x = Group, y = Glu)) +
  geom_boxplot() +
  labs(x = "Status", y = "Glucose") +
  ggtitle("Glucose Distribution by Diabetes Status") +
  theme_minimal()

ggplot(combined_data, aes(x = Group, y = Glu, fill = Group)) +
  geom_violin(trim = FALSE) +
  labs(x = "Status", y = "Glucose") +
  ggtitle("Glucose Distribution by Diabetes Status") +
  theme_minimal()
```

It is clear that the glucose distribution after fasting for the two groups have vastly different variances, thus a two-sample t-test would not be applicable due to its equal variance assumption. Further, it appears that the two distributions follow somewhat the parametric assumption. As a sanity check we can setup a test of the to distributions to verify that the variances should not be equal.

To achieve this we can perform an F-test which will test whether the variances of the two groups are equal. Thus, we will have the following hypotheses (where $d$ represents the diabetics and $\bar{d}$ represents the non-diabetics):

$$
\begin{align}
  H_0 &: \sigma_{d} = \sigma_{\bar{d}} \\
  H_1 &:\sigma_{d} \neq \sigma_{\bar{d}} \hspace{5mm} \text{for a two-tailed F-test}
\end{align}
$$

Since, we are only interested to see if the variances are unequal or not we use the two-tailed alternative hypothesis for the F-test.

```{r}
var.test(diabetic$Glu, non_diabetic$Glu, alternative = "two.sided")
```

Indeed, we can report a p-value of 2.2e-16 thus, we can reject the null hypothesis and conclude that the variances are indeed different. Which leads us to using the Welch student t-test which assumes parametric distribution with two groups of different variances. Again we use the same two sided alternative hypothesis but now for the means.

$$
\begin{align}
H_0 &: \mu_{d} = \mu_{\bar{d}} \\
H_1 &: \mu_{d} \neq \mu_{\bar{d}}
\end{align}
$$

With the following piece of code we can perform this Welch t-test on the means.

```{r}
t.test(diabetic$Glu, non_diabetic$Glu, alternative="two.sided")
```

Given that $\alpha = 0.05$ we can reject the null-hypothesis and say that our results of this test is significant and shows that we do not have enough evidence to conclude that the means between the two groups are indeed the same.

Furthermore, it is worth noting that the sample size between the two groups are different and thus the traditional t-test mechanisms are changed to account for this. However, the techniques used above will be able to handle this internally.

### 1.2 Diabetic mean blood glucose level between adults and seniors

**Task description:** Test for the diabetic respondents if the mean level of blood glucose after fasting is the same for adults and seniors.

To analyse what test will be the most applicable we will first plot the distributions, but to do that we will first need to create these two groups from our dataset.

```{r}
diabetic_adult <- data %>% filter(Diabete == 1 & age_group == "Adult")
diabetic_seniors <- data %>% filter(Diabete == 1 & age_group == "Senior")

rmarkdown::paged_table(diabetic_adult)
rmarkdown::paged_table(diabetic_seniors)

diabetic_adult$Group <- "Diabetic Adult"
diabetic_seniors$Group <- "Diabetic Senior"

combined_data <- rbind(diabetic_adult, diabetic_seniors)
rmarkdown::paged_table(combined_data)
```

We then use the new combined dataframe consisting only of these two groups to plot the distributions.

```{r}
ggplot(combined_data, aes(x = Group, y = Glu)) +
  geom_boxplot() +
  labs(x = "Age Group", y = "Glucose") +
  ggtitle("Glucose Distribution for Diabetics by Age Group") +
  theme_minimal()

ggplot(combined_data, aes(x = Group, y = Glu, fill = Group)) +
  geom_violin(trim = FALSE) +
  labs(x = "Age Group", y = "Glucose") +
  ggtitle("Glucose Distribution for Diabetics by Age Group") +
  theme_minimal()
```

From this we can observe that the distribution for the diabetic adults and diabetic seniors do not have the same variance and that they do not follow the parametric assumption. Which is clear by the fact that for seniors we have two distinct peaks which indicate that they do not follow the parametric distribution. Making the two sample Welch t-test not applicable in this scenario. Thus, to circumvent these limiting assumptions in this case we will be performing a Wilcoxon test which does not have an underlying parametric assumption about the distribution of the data.

Given the task description we can setup the following hypothesis test. Where $a$ represents the adult group and $s$ represents the senior group.

$$
\begin{align}
H_0 &: \mu_{a} = \mu_{s} \\
H_1 &: \mu_{a} \neq \mu_{s}
\end{align}
$$

Thus, we can perform the following Wilcox test to test this hypothesis.

```{r}
delta_wilcox <- wilcox.test(diabetic_seniors$Glu, diabetic_adult$Glu, alternative="two.sided")
delta_wilcox
```

Based on these results we can observe that we cannot reject the null-hypothesis, thus, we can say that there is not enough evidence to suggest that the mean blood glucose between diabetic adults and diabetic seniors is different.

### 1.3 Vigorous work activity mean blood glucose for adult diabetics

**Task description:** Is it possible to test for the adults diabetic respondents if the mean level of blood glucose after fasting is the same for those who have a vigorous work activity and for those who have not?

Again we will first filter the dataset and create a new combined dataset for these two groups.

```{r}
diabetic_active <- data %>% filter(Diabete == 1 & age_group == "Adult" & Phys_activ == 1)
diabetic_inactive <- data %>% filter(Diabete == 1 & age_group == "Adult" & Phys_activ == 2)

diabetic_active$Group <- "Diabetic Active"
diabetic_inactive$Group <- "Diabetic Inactive"

rmarkdown::paged_table(diabetic_active)
rmarkdown::paged_table(diabetic_inactive)

# Combine the data frames
combined_data <- rbind(diabetic_active, diabetic_inactive)
rmarkdown::paged_table(combined_data)

ggplot(combined_data, aes(x = Group, y = Glu)) +
  geom_boxplot() +
  labs(x = "Activity Group", y = "Glucose") +
  ggtitle("Glucose Distribution for Diabetics by Activity") +
  theme_minimal()

ggplot(combined_data, aes(x = Group, y = Glu, fill = Group)) +
  geom_violin(trim = FALSE) +
  labs(x = "Activity Group", y = "Glucose") +
  ggtitle("Glucose Distribution for Diabetics by Activity") +
  theme_minimal()
```

Immediately we can observe that we have significantly less data than for the other exercises, which can indicate that test results can be inconclusive. Furthermore, we cannot necessarily make an assumption about the normality of the data due to the fact that we have such little samples. It could for example be the case that for many more samples that the distribution looks much more like a normal.

However, for now we will not make any normality assumption and we will be using a Wilcoxon test where we will analyse its power of varying number of samples for $\alpha=0.05$ which will enable us to see how the power of the test evolves over the number of samples.

```{r}
delta_wilcox  <- wilcox.test(diabetic_active$Glu, diabetic_inactive$Glu, alternative="two.sided")$statistic
alpha <- 0.05
seq_n <- seq(10,1)
power_wilcox  <- map_dbl(seq_n, ~power.t.test(n = .x, delta_wilcox , sig.level = alpha)$power)
data.frame(power = c(power_wilcox), n = rep(seq_n),
           method = rep(c("Wilcox"), each = length(seq_n))) %>% 
  ggplot() + aes(x = n, y = power, color = method) + geom_line()
```

We observe that for a sample size of 4, the power of the Wilcoxon t-test is more or less 1.0 meaning that it is almost guaranteed that the null-hypothesis will be rejected when it is false. Furthermore, we plot for $n=4$ the different values of $\alpha$ wrt. the power of the Wilcoxon test.

```{r}
delta_wilcox  <- wilcox.test(diabetic_active$Glu, diabetic_inactive$Glu, alternative="two.sided")$statistic
seq_alpha <- 10^seq(-12, -5, .1)
n <- 4
power_wilcox  <- map_dbl(seq_alpha, ~power.t.test(n, delta_wilcox , sig.level = .x)$power)
data.frame(power = c(power_wilcox), alpha = rep(seq_alpha, 1),
           method = rep(c("Wilcox"), each = length(seq_alpha))) %>% 
  ggplot() + aes(x = alpha, y = power, color = method) + geom_line()
```

As observed earlier for a $\alpha=0.05$ we basically have guaranteed that the null-hypothesis is rejected correctly only after 2 samples.

The conclusion is that th Wilcoxon test breaks down with so few samples. The Wolcoxon table, which is used to obtain the $\alpha$, does not go below a sample size of 5. And in the case that all the values of one distribution is larger, it gives at most an $\alpha$ of 0.1.\
The other case would be to try and fit the data to a distribution. But with so few datapoints, that would be futile.\
The answer to the question is therefore no, one can not make any conclusions.

### 1.4 Same proportion of male and female diabetics

**Task description:** Test if the proportion of diabetic is the same for male and female respondents. Compare conclusions of several tests. You can use the function `table` to compute contingency tables.

First we compute the contingency table which we will use to be able to test whether the proportion of diabete males and females are the same.

```{r}
diabete_and_nondiabete = filter(data, Diabete != 3)
contingency_table <- table(diabete_and_nondiabete$Sex, diabete_and_nondiabete$Diabete)
contingency_table
```

One method is the Fisher test, which does not rely on sample sizes and can compute p-values exactly. Where we have the following hypotheses:

$$
H_0: \hspace{5mm} P_{\text{female-diabetic}} =  P_{\text{male-diabetic}} \\
H_1: \hspace{5mm} P_{\text{female-diabetic}} \neq  P_{\text{male-diabetic}}
$$

```{r}
fisher.test(contingency_table)
```

Indeed we do not observe that the null-hypothesis is rejected. However, we do note an odds ratio of around 64.7% thus it is slightly more expected for one group to be assigned the diabete label. But again the p-value is not significant enough to draw any conclusions.

Further, one could assume some form of normal approximation of the data and in that case we can apply a 2-sample test for the equality of proportions. Which assumes that the difference between the proportions can be drawn from the following normal.

$$
\hat{p}_A - \hat{p}_B \sim \mathcal{N}\left(0, p(1-p) \left( \dfrac{1}{n_A} + \dfrac{1}{n_B} \right) \right)
$$

Which we can formulate through the following test.

```{r}
diabetes = c(8, 13)
population = c(nrow(filter(data, Sex==1)), nrow(filter(data, Sex==2)))

prop.test(diabetes, population, correct = FALSE)
```

Again we cannot reject the null-hypothesis as the p-value is not significant. Another approach which builds on top of this is the Yates continuity correction which subtracts 0.5 from the difference between each observed value to lower the chi-squared value and thus boost the p-value.

```{r}
prop.test(diabetes, population, correct = TRUE)
```

As this test implicitly increases the p-value we of course cannot expect to reject the null-hypothesis with this test given the results above. Therefore, from the collection of tests above we cannot reject the null-hypothesis and can conclude that the proportion of male and female diabetes are the same.

## 2. Gene expression data

```{r}
load("liver_data.rda")
```

## 2.1 Finding most significant discovery

**Task description:**: We first want to study the gene which is the most correlated with the level of cholesterol. Identify this gene and plot cholesterol as a function of this gene expression level. Fit a linear model. Test if there is a significant relation between this gene expression and cholesterol?

We first want to study the gene which is the most correlated with the level of cholesterol. Identify this gene and plot `cholesterol` as a function of this gene expression level. Fit a linear model. Test if there is a significant relation between this gene expression and `cholesterol`?

```{r}
labels <- colnames(liver)

maxSlope=0
minP=1

for (i in 2:3117){
  lmgene <- lm(cholesterol~liver[,i], data=liver)
  slope <- abs(coef(lmgene, complete = TRUE)[2])
  summary_stats <- summary(lmgene)
  p_value <- summary_stats$coefficients[2, 4]
  
  if (slope > maxSlope){
    maxSlope=slope
    max_slope_model=lmgene
  }
  
  if (minP > p_value){
    minP=p_value
    min_p_model=lmgene
    genename=labels[i]
  }
}

```

Fitting a linear model for every gene, we get 3116 different models in total. The null-hypothesis is that a gene does no have a correlation with the cholesterol-level, in other words, the slope is zero in this case. Consequently, the gene corresponding to the model with the lowest p-value is also the model with highest slope. In our case, that turned out to be gene $A\_42P631473$ with a p-value of less than $10^{-13}$.

```{r}

ggplotRegression <- function (fit) {

require(ggplot2)

ggplot(fit$model, aes_string(x = names(fit$model)[2], y = names(fit$model)[1])) + 
  geom_point() +
  stat_smooth(method = "lm", col = "red") +
  labs(title = paste("Adj R2 = ",signif(summary(fit)$adj.r.squared, 5),
                     "Intercept =",signif(fit$coef[[1]],5 ),
                     " Slope =",signif(fit$coef[[2]], 5),
                     " P =",signif(summary(fit)$coef[2,4], 5)), x = paste(genename, "-expression"))
}

ggplotRegression(min_p_model)
#ggplotRegression(max_slope_model)


```

## 2.2 Calculating all p-values

**Task description:**: We now wish to perform the previous test for all of the 3116 genes. For each of the genes, fit a linear model that explains cholesterol as a function of the gene expression and compute the p-value of the test.

We now do the same test for all 3116 genes and fit a linear model that explains `cholesterol` as a function of the gene expression. Doing so gives us a relatively large proportion of genes that correspond to a p-value of less than 0.05

```{r}
labels <- colnames(liver)

p_list <- c()
lineList = c()

for (i in 2:3117){
  lmgene <- lm(cholesterol~liver[,i], data=liver)
  slope <- abs(coef(lmgene, complete = TRUE)[2])
  summary_stats <- summary(lmgene)
  p_value <- summary_stats$coefficients[2, 4]
  p_list=c(p_list, p_value)
  lineList = c(lineList, (i-1)/3116)
}

```

```{r}
plot(hist(p_list, breaks=20, col = c("orange", rep("lightblue", 100))), main = "Density of p-values", xlab = "p-values")

```

## 2.3 Ordering p-values

**Task description:**Order the p-values and plot the ordered as a function of their rank. On the same plot, display the line y=x/3116. Discuss the result.

Plotting all values, we can conclude that there are amount of models that have a p-value below 0.05. It is tempting to make the conclusion that all these models are statistically significant since they all have a small p-value. However, in the event that the we only have random data, we still expect 5% of the models to appear significantly significant. In fact, if we were to only have random data, we expect a uniform distribution between 0 and 1 for the p-values. Visually, looking at the figure above, we can the p-values perform better that the uniform distribution and therefore appear to contain significant discoveries.

```{r}

p_list=sort(p_list)
plot(p_list, col = "blue", lwd = 2, xlab = "Rank", ylab = "p-value")
lines(lineList, col = "red", lwd = 2)

legend("topleft", legend = c("p-values", "Uniform line"), col = c("blue", "red"), lwd = 2)


```

Shuffling all labels, we get an approximate uniform distribution of p-values which corresponds to the p-values following a straight line

```{r}
labels <- colnames(liver)

p_list_shuffeled <- c()
lineList_shuffeled = c()

for (i in 2:3117){
  cholesterol=liver$cholesterol
  randomLabel <- sample(cholesterol)
  lmgene <- lm(randomLabel~liver[,i], data=liver)
  slope <- abs(coef(lmgene, complete = TRUE)[2])
  summary_stats <- summary(lmgene)
  p_value <- summary_stats$coefficients[2, 4]
  p_list_shuffeled=c(p_list_shuffeled, p_value)
  lineList_shuffeled = c(lineList_shuffeled, (i-1)/3116)
}

p_list_shuffeled=sort(p_list_shuffeled)
plot(p_list_shuffeled, col = "blue", lwd = 2, xlab = "Rank", ylab = "p-value", main = "Shuffeled labels")
lines(lineList_shuffeled, col = "red", lwd = 2)

legend("topleft", legend = c("p-values when shuffeled labels", "Uniform line"), col = c("blue", "red"), lwd = 2)

plot(hist(p_list_shuffeled, breaks=20, col = c("orange", rep("lightblue", 100))), main = "Density of p-values", xlab = "p-values")

```

## 2.4 Benjamini-Hochberg procedure

**Task description:**Identify a set of genes linked to the response (aka discoveries). We want to guarantee that the expected proportion of false discoveries (mistakes) is less than 5%. Explain how you proceed and how many genes you discover.

In the nest question, we are tasked with finding the maximum number of genes such that the expected number of false discoveries is at most 5%. For this method, we use the Benjamini-Hochberg procedure \cite{FDR}. In short, it states that if we have a set of $\{ H_1, H_2, ... \: , H_m \}$ with associated and ordered p-values $\{ p_1, p_2, ... \: , p_m \}$, then the threshold for the index can be written as:

$$ P_{BH}= max_i \left\{ P_{(i)} : P_{(i)} \leq \alpha \frac{i}{m}      \right\} $$

```{r}

soft_guarantee=c()
alpha=0.05
index=0
n=3116



for (i in 1:3116){
  val=i*alpha/n
  soft_guarantee=c(soft_guarantee, val)
  if (p_list[i]<val){
    index=i
  }

}



plot(p_list, col = "blue", lwd = 2, xlab = "Index", ylab = "p-value")
lines(soft_guarantee, col = "red", lwd = 2)
abline(v=index, col="black")
legend("topleft", legend = c("Benjamini-Hochberg boundary", "P-value", paste("Intersect=", as.character(index))), col = c("red", "blue", "black"), lwd = 2)

plot(p_list[1:1200], col = "blue", lwd = 2, xlab = "Index", ylab = "p-value")
lines(soft_guarantee[1:1200], col = "red", lwd = 2)
abline(v=index, col="black")
legend("topleft", legend = c("Benjamini-Hochberg boundary", "P-value", paste("Intersect=", as.character(index))), col = c("red", "blue", "black"), lwd = 2)


```

We can see that $\alpha \frac{i}{m}$ plots out a line, and we choose i such that $P_{(i)}$ always is smaller or equal to that value. Doing so, we get 941 discoveries such that the expected number of false discoveries is no more than 5 %

##2.5 Holm–Bonferroni method **Task description:** We wish to be more conservative and guarantee that the probability of making a false discovery (or more) is less than 5%. Explain how you proceed and how many genes you discover.

A stronger guarantee would be that on impose that there shouldn't be more than a 5 % risk that any discovery is false. For this method, we use the Holm–Bonferroni method. The method is as follows:

I have my ordered p-values, $\{ P_1, P_2, ... \: , P_m \}$, and want the FWER to be no higher than a level $\alpha$.

1.  Is $P_1 < \alpha/m$? Reject $H_1$ and continue to the next step, otherwise EXIT.

2.  Is $P_2 < \alpha/(m-1)$? so, reject $H_2$ and continue to the next step, otherwise EXIT.

3.  Continue this iterative process as long as $P_k < \frac{\alpha}{m+1-k}$.

    Using this process, we get the following result:

```{r}

strong_guarantee=c()
alpha=0.05
index=0

for (i in 1:n){
  val=alpha/(n+1-i)
  strong_guarantee=c(strong_guarantee, val)
  if (p_list[i]<val){
    index=i
  }

}


plot(p_list, col = "blue", lwd = 2, xlab = "Index", ylab = "p-value")
lines(strong_guarantee, col = "red", lwd = 2)
abline(v=index, col="black", )
legend("topleft", legend = c("Holm–Bonferroni", "P-value", paste("Intersect=", as.character(index))), col = c("red", "blue", "black"), lwd = 2)


plot(p_list[1:225], col = "blue", lwd = 2, xlab = "Index", ylab = "p-value")
lines(strong_guarantee, col = "red", lwd = 2)
abline(v=index, col="black", )
legend("bottomright", legend = c("Holm–Bonferroni", "P-value", paste("Intersect=", as.character(index))), col = c("red", "blue", "black"), lwd = 2)



```

Using this process, we see that we get 223 discoveries. We also not that smallest p-value in that set is approximately equal to $1.7 \cdot 10^{-5}$.

## 3. Non parametric regression


### 3.1 Data uploading and plotting

**Task description:** Upload the dataset contained in the file data_exo3.csv and plot the data.

The first task asks to read and save the dataset contained in data_exo3.csv.

```{r}
data = read.csv("data_exo3.csv", sep="t", dec=",", na="")
```

It's a two-dimensional dataset in two variables denoted as x and y. After this, we want to plot the data on the plane:

```{r}
plot <- ggplot(data, aes(x = x, y = y)) + 
  geom_point() + 
  labs(title = "Scatter plot of the data", x = "X", y = "Y")
plot
```

It is evident from the data exploration that observations with lower values of x are situated towards the upper region of the graph, whereas conversely, as x increases, the data points tend to shift lower. In the middle range, there is a discernible transitional pattern. 

### 3.2 Polynomial Regression Models

**Task description:** Try several polynomial models for the data, select a model and comment the results.

In this section, we embark on the exploration of polynomial regression models. Our objective is to discern and select an appropriate polynomial model that best encapsulates the relationship between the variables.

We initiate by fitting polynomial models ranging from degree 1 to 5. This choice is motivated by the desire to strike a balance between model simplicity and the ability to flexibly adapt to nuanced patterns within the data.

But first, let's split the data in training and test in order to evaluate the predictive performance of the model.


```{r}
n <- nrow(data)
is_train <- sample(c(TRUE, FALSE), n, prob = c(0.8, 0.2), replace = TRUE)
training <- filter(data,  is_train)
test     <- filter(data, !is_train)
```

Starting with the simplest model, a linear regression (degree 1), we aim to assess how well a straight line aligns with the observed data points.

```{r}
library(ggfortify)
```

```{r}
lm1 <- lm(y ~ x, data = training)
summary(lm1)

predictions  <- 
  lm(y ~ x, data = training) %>% 
  predict(test)

pred <- predictions

R2_test <- cor(predictions, test$y)^2
print("R2 test: ")
R2_test

plot <-
  training %>% ggplot() +  aes(x = x, y = y) + 
  geom_point(size = 2, colour="#993399") +  xlab("x") + ylab("y") +
  geom_point(data = data.frame(test, testing = predictions),
             mapping = aes(x = x, y = y), size = 2, shape = 3, colour="red")

plot <- plot + 
  geom_smooth(method = "lm", data=training, formula = y ~ x, se = FALSE, colour="#339900") 
plot

autoplot(lm1, which = 1:2) 
```

The model exhibits a commendable R-squared value of 0.79, signifying a strong fit, and the F-statistic indicates a highly significant overall model (p<0.001). The score on the test dataset is slightly lower.

We can see by the diagnostic plots that the model seems to underestimate the data in some intervals and overestimate in others. The residuals are not identically distributed around 0, as they follow a trend, and the extreme value are not extreme value of a normal distribution. A good idea is considering a polynomial regression of degree 2.

```{r}
lm2 <- lm(y ~ poly(x, 2), data=training)
summary(lm2)

predictions  <- 
  lm(y ~ poly(x, 2), data = training) %>% 
  predict(test)

R2_test <- cor(predictions, test$y)^2
print("R2 test: ")
R2_test

plot <- plot + 
  geom_smooth(method = "lm", data=training, formula = y ~ poly(x, 2, raw = TRUE), se = FALSE, colour="#177711")
plot

autoplot(lm2, which = 1:2)
```

The 2nd degree polynomial model shows a great improvement as it explains 89 percent of the variability of the training data and scores 87 percent on the test data. The plot shows how the curve better estimate the data and the diagnostic plot shows how the residuals' trend is closer to the axis.

```{r}
lm3 <- lm(y ~ poly(x, 3), data=training)
summary(lm3)

predictions  <- 
  lm(y ~ poly(x, 3), data = training) %>% 
  predict(test)

R2_test <- cor(predictions, test$y)^2
print("R2 test: ")
R2_test

plot <- plot + 
  geom_smooth(method = "lm", data=training, formula = y ~ poly(x, 3, raw = TRUE), se = FALSE, colour="#705511")
plot

autoplot(lm3, which = 1:2)
```

The 3rd degree polynomial model slightly improves the performance both on the training and the test data. On the other hand, there's no confirm on the statistical significance of the 3rd degree component, due to the p-value of 0.16.By the diagnostic plots, we observe that the distribution of residuals is still unbalanced.

```{r}
lm4 <- lm(y ~ poly(x, 4), data=training)
summary(lm4)

predictions  <- 
  lm(y ~ poly(x, 4), data = training) %>% 
  predict(test)

R2_test <- cor(predictions, test$y)^2
print("R2 test: ")
R2_test

plot <- plot + 
  geom_smooth(method = "lm", data=training, formula = y ~ poly(x, 4, raw = TRUE), se = FALSE, colour="#723366")
plot

autoplot(lm4, which = 1:2)
```

The 4th degree polynomial model shows a much better performance, explaining 95 percent of the variability of the data. It also has a high predictive power with a 97.7 percent score on the test data. The Residual vs Fitted diagnostic plot shows that the trend is closer to zero in this case.

```{r}
lm5 <- lm(y ~ poly(x, 5), data=training)
summary(lm5)

predictions  <- 
  lm(y ~ poly(x, 5), data = training) %>% 
  predict(test)

R2_test <- cor(predictions, test$y)^2
print("R2 test: ")
R2_test

plot <- plot + 
  geom_smooth(method = "lm", data=training, formula = y ~ poly(x, 5, raw = TRUE), se = FALSE, colour="#720099")
plot

autoplot(lm5, which = 1:2)
```

Finally, the fifth grade polynomial model increases slightly the score on the training data from 96 to 97 percent, decreasing the predictive model performance. It can be a consequence of overfitting. The fifth grade component is not confirmed to be statistically significant due to the p-value of 0.22. Overall, it seems less accurate than the previous model.

From the previous observations, we have little reasons to prefer the 4th grade polynomial model. Let's delve into statistical tests in order to quantitatively verify what we observed.

```{r}
anova(lm1,lm4)
anova(lm1,lm5)
anova(lm2,lm4)
anova(lm3,lm4)
anova(lm2,lm5)
anova(lm3,lm5)
```

As we observed during the single models analysis, the 4th and 5th grade models are highly preferred to the lower grade ones.

```{r}
anova(lm4,lm5)
```

By comparing 4th and 5th grade polynomials, the test doesn't let us reject the hypothesis that the 4th grade polynomial is enough to explain the data (instead of using the 5th grade component). We can go on with the likelihood ratio test, since out five model are nested with each other.

```{r}
logLik(lm1)
logLik(lm2)
logLik(lm3)
logLik(lm4)
logLik(lm5)
```

The test prefer the 4th and 5th grade polynomials as well, and indicates a little preference for the 4th grade one. The difference is not big enough to conclude that the 4th grade model better describes the relations among the variables of the data set. We can pursue our investigation by using information criteria

```{r}
as.matrix(BIC(lm1, lm2, lm3, lm4, lm5))
as.matrix(AIC(lm1, lm2, lm3, lm4, lm5))
```

The BIC test leans marginally towards favoring the 4th-grade model, whereas the AIC test yields nearly equivalent values for the 4th and 5th-grade polynomial models.

Considering the inclinations of the AIC and likelihood ratio tests, coupled with the superior prediction score on the test data and the lack of confirmed statistical significance in the 5th-grade component, we adhere to the 4th-grade polynomial regression model as the most suitable polynomial model.

### 3.3 Nonlinear Regression Model and Comparison

**Task description:** Fit a nonlinear model and compare with the polynomial model you have selected.

Given the discernible shape of the data from the plot, a viable approach involves attempting to model the data using the following function:
y ~ B + A / ( 1 + exp(- gamma * (x -tau)))
This function incorporates four parameters: A, B, gamma and tau, which are subject to estimation.

```{r}
library(minpack.lm)
nlm1 <- nlsLM(y ~  B + A / ( 1 + exp(- gamma * (x -tau))), data = training, start = c(A=0, gamma=0, tau=2, B=2))
summary(nlm1)

predictions <- predict(nlm1, newdata = test)

R2_test <- cor(predictions, test$y)^2
print("R2 test: ")
R2_test
```

This nonlinear model contains a better description of the data and a quasi-perfect prediction performance with a 99.5 percent score.

```{r}
new_data <- data.frame(x = seq(min(data$x), max(data$x), length.out = 100))

predictions <- predict(nlm1, newdata = new_data)

library(ggplot2)

plot <-
  training %>% ggplot() +  aes(x = x, y = y) + 
  geom_point(size = 2, colour="#993399") +  xlab("x") + ylab("y") +
  geom_point(data = data.frame(test, testing = pred),
             mapping = aes(x = x, y = y), size = 2, shape = 3, colour="red")

plot2 <- plot +
  geom_line(data = new_data, aes(x = x, y = predictions), color = "blue") +
  labs(title = "Fit del Modello nls", x = "X", y = "Y")

plot2

plot2 +
  geom_smooth(data = data, 
    mapping = aes(x = x, y = y),
    method = "lm", formula = y ~ poly(x, 4, raw = TRUE), se = FALSE, color = "red")
```

By visual examination, it's evident that the nonlinear model better describes and estimates the data within the range x=3 to x=6. It is anticipated that subsequent statistical tests will quantitatively affirm the preference for the nonlinear model when compared to the polynomial counterpart.

```{r}
logLik(lm4)
logLik(nlm1)
```

The likelihood ratio test unequivocally supports the superiority of the nonlinear model, showcasing a substantial advantage over its counterpart.

```{r}
as.matrix(BIC(lm4, nlm1))
as.matrix(AIC(lm4, nlm1))
```

The outcome is affine when evaluating and comparing using information criteria. We have enough evidence to conclude that the preferred model for the description of the dataset is the nonlinear one.

### 3.4 Confidence Intervals and Prediction Intervals

**Task description:** Calculate confidence intervals for E(y_new) and prediction intervals for Y_new on the following values of the covariate: x_new = (1,...,10)

In the context of obtaining prediction intervals and confidence intervals, we employ the delta-method.

```{r}
# Calculate the derivative
f_prime <- deriv(y ~  B + A / ( 1 + exp(- gamma * (x -tau))), c("A", "gamma", "tau", "B"), function(A, gamma, tau, B, x){} ) 

# Generate x_new
x_new <- seq(1, 10, by=1)

# Obtain the coefficients from the nonlinear model
beta_hat <- coef(nlm1)

# Calculate the new function values and gradient
f_new    <- f_prime(beta_hat[1], beta_hat[2], beta_hat[3], beta_hat[4], x_new)
grad_new <- attr(f_new, "gradient")
GS <- rowSums((grad_new %*% vcov(nlm1)) * grad_new)

# Set the confidence level
alpha <- 0.05

# Calculate the confidence intervals and create a data frame
delta_f <- sqrt(GS) * qt(1-alpha/2,  n - length(beta_hat))
df_delta <- data.frame(
  x = x_new, 
  f = f_new, 
  lwr_conf = f_new - delta_f, 
  upr_conf = f_new + delta_f
  )

# Calculate prediction intervals and add to the data frame
delta_y    <- sqrt(GS + sigma(nlm1)^2)*qt(1-alpha/2,  n - length(beta_hat))
df_delta[c("lwr_pred","upr_pred")] <- cbind(f_new - delta_y,f_new + delta_y)

# Display the resulting data frame
df_delta

# Plot the results
ggplot(df_delta, aes(x = x, y = f)) +
  geom_point() +
  geom_ribbon(aes(ymin = lwr_conf, ymax = upr_conf), fill = "blue", alpha = 0.2, color = NA, linetype = 0) +
  geom_ribbon(aes(ymin = lwr_pred, ymax = upr_pred), fill = "red", alpha = 0.2, color = NA, linetype = 0) +
  labs(title = "Confidence Intervals and Prediction Intervals",
       x = "x_new",
       y = "y_new") +
  theme_minimal()
```

In this visualization, the model's predictions are denoted by black points, accompanied by the respective prediction interval and confidence interval. These intervals have been computed for each value of x_new and connected to form two continuous bands encapsulating the forecasted outcomes.

## 4. S&P500 daily return

### 4.1 Data loading and computing daily return

**Task description:** Load the dataset `sp500_history.csv` into a *data frame* and add a column that computes the daily return for each days.

```{r}
sp500_data <- read_csv("sp500_history.csv")
sp500_data$DailyReturn <- sp500_data$Close - sp500_data$Open

# Check for non-numeric values in DailyReturn column
non_numeric_count <- sum(is.na(sp500_data$DailyReturn) | !is.numeric(sp500_data$DailyReturn))
cat("Number of non-numeric values in DailyReturn column:", non_numeric_count, "\n")

if (non_numeric_count > 0) {
  cat("Indices of non-numeric values in DailyReturn column:", which(!is.numeric(sp500_data$DailyReturn)), "\n")
} else {
  cat("No non-numeric values found in DailyReturn column.\n")
}

rmarkdown::paged_table(sp500_data)
```

### 4.2 Daily return sampled from a normal distribution

**Task description:** We propose to model the daily return as a sample from a normal population. Write the model and use *R* to fit it to the data. What do you think of this model?

To first understand the data, we plot the daily return as a function of the day number.

```{r}
ggplot(sp500_data, aes(x = Date, y = DailyReturn)) +
geom_histogram(stat = "identity", fill = "grey", color = "grey", bins = 15) +
labs(title = "Daily Return of S&P 500 (2007-2010)",
     x = "Date",
     y = "Daily Return") +
theme_minimal() +
theme(axis.text.x = element_blank(),
      axis.title.x = element_blank())
```

Under Louis Bachelier's "Theory of Speculation" one assumes that the returns can be sampled from a perfectly random distribution, in this case this would be a normal distribution which we can parametirise through a Gaussian model. Assume that the return is modeled by some random variable $X_r$ and that some value of that RV is denoted by $x_t$. Thus, since we first assume Bachelier's theory we can model the RV through the following Gaussian.

$$    X_r \sim \mathcal{N}(\mu_r, \sigma_r^2)    $$ Where the model parameters are the empirical mean and empirical variance expressed by: $$    \mu_r = \dfrac{\sum_{i=0}^N x_i}{N} \hspace{5mm} \sigma_r^2 = \dfrac{\sum_{i=0}^N (x_i - \mu_r)}{N}    $$ We can then directly fit such a Gaussian model in R through the following:

```{r}
fit <- fitdistr(sp500_data$DailyReturn, "normal")
fit

# Plot the histogram of daily returns with the fitted Gaussian distribution
hist(sp500_data$DailyReturn, breaks = 100, freq = FALSE, 
     main = "Daily Returns with Fitted Gaussian Distribution",
     xlab = "Daily Return")
curve(dnorm(x, mean = fit$estimate[1], sd = fit$estimate[2]), 
      col = "blue", lwd = 2, add = TRUE, yaxt = "n")
```

Indeed, the it seems that Bachelier's theory does describe the price movements to some extent. However, we do observe that that tails seem to follow a higher variance compared to the density around the mean which seems to have a smaller variance. Which also explains why our initial Gaussian doesn't effectively capture this distribution. Thus, we can motivate why a Gaussian mixture model might perform better in this scenario.

### 4.3 Gaussian Mixture Model

**Task description:** Instead of a single normal, we now propose to use a mixture of normals with $p$ components. Write the corresponding models and use *R* to fit them to the data for all values of $p$ between $2$ and $6$. Discuss the results and compare them with the model of Question 1.

To combine the multiple Gaussians to model a single valid PDF, we will have to introduce a weight parameter which will be used on the weighted sum of Gaussian components. First, we denote a single component the following way. $$    Y_r \sim \mathcal{N}(\theta_k) \hspace{5mm} \text{Where $\theta$ is a parameter vector for component $k$ defined by} \hspace{5mm} \theta_k = \left[ \mu_k, \sigma_k^2 \right]    $$ Furthermore, to form the mixture we introduce the weight parameter $\pi_k$ which is probabilistic and forms the weighted sum (GMM). $$    Y_i = \sum_{k=1}^K \pi_k \mathcal{N}(\theta_k)    $$ As this is a weighted sum and we expect $Y_i$ to be modeled from a valid PDF, we have to constrain the weights such that: $$    \pi_k \geq 0 \hspace{2.5mm} \land \hspace{2.5mm} \sum_{k=1}^K \pi_k = 1    $$ Which is the case as long as we follow that $\pi_k$ comes from a PDF defined over the categorical RVs $Z_i$ which defines the different components such that we have: $$    P(Z_i = k) = \pi_k \hspace{5mm} \text{Where $Z$ is a sequence of RVs} \hspace{5mm} \{1, 2, \dots, K\}    $$ Thus, we observe that we first have to initialize the parameters of the model which we can do by placing the means based on a K-Means initialization and set some explicit variance which will serve as the initialization for the iterative EM-algorithm for parameter optimization over such a GMM. First off, we will naively apply a GMM routine to this problem using expectation maximization which we will compare against a constrained optimization approach where we ensure equal means for all components as it seems to only be the variance that's needed to change based on our earlier graph.

#### 4.3.1 Unconstrained EM Optimization of GMMs

For this set of experiments we will be using the general GMM approach outlines above, where we initialize the means using k-Means and perform EM to optimize the model parameters.

```{r}
n_components = 3
y = sp500_data$DailyReturn

kmeans_out <- kmeans(y, centers = n_components)
mu = as.vector(kmeans_out$centers)
mu
```

This results seems strange given that we should want Gaussian components around the same mean. However, for this experiment we will naively apply k-Means initialization without any constraints in the optimization and observe what results we obtain.

```{r}
# Function which fits n gaussian components and plots the components with the final mixture density.
fit_mixture_model <- function(n_components, y, max_iter = 400) {
  kmeans_out <- kmeans(y, centers = n_components)
  mu <- as.vector(kmeans_out$centers)
  mixture_EM <- normalmixEM(y, mu = mu, k = n_components, arbvar = FALSE, maxit = max_iter)
  log_likelihood <- mixture_EM$loglik
  
  result <- list(pi = mixture_EM$lambda, mu = mixture_EM$mu, sigma = mixture_EM$sigma)
  
  # Plot histogram with 50 bars
  hist(y, breaks = 50, probability = TRUE, col = alpha("skyblue", 0.25), border = alpha("black", 0.25), main = paste("Mixture Model with", n_components, "Components"), xlab = "Daily Return", ylab = "Density")
  
  # Add density line
  final_density <- rowSums(sapply(1:length(mixture_EM$lambda), function(i) mixture_EM$lambda[i] * dnorm(x, mean = mixture_EM$mu[i], sd = mixture_EM$sigma[i])))
  
  x <- seq(min(y), max(y), length.out = 1000)
  lines(x, final_density, lwd = 2)
  
  # Plot densities of Gaussian components
  for (i in 1:n_components) {
    curve(result$pi[i] * dnorm(x, mean = result$mu[i], sd = result$sigma[i]), add = TRUE, col = i + 1, lwd=2, lty=2)
  }
  
  return(list(result = result, mixture_EM = mixture_EM, log_likelihood = log_likelihood))
}
```

Using the above function, we can plot the resulting fits for $2$ to $6$ Gaussian components.

```{r}
mx2 <- fit_mixture_model(2, y)
mx3 <- fit_mixture_model(3, y)
mx4 <- fit_mixture_model(4, y)
mx5 <- fit_mixture_model(5, y)
mx6 <- fit_mixture_model(6, y)

mixtures = c(mx2, mx3, mx4, mx5, mx6)
```

We can observe that the final fits are indeed much better than the single Gaussian example. However, we do observe that there is some over fitting happening at the tails of the distribution for all of the experiments we have run. This is caused by the fact that the means are not the same for all the Gaussian components and thus for our next section we will be looking at constrained optimization of this GMM where the means should be the same.

```{r}
bic <- function(model) {
  n_free_params <- length(mx2$result$pi)
  n <- length(model$mixture_EM$x)
  
  return(-2 * model$log_likelihood + log(n) * n_free_params)
}

rbind(
  gmm_2 = bic(mx2),
  gmm_3 = bic(mx3),
  gmm_4 = bic(mx4),
  gmm_5 = bic(mx5),
  gmm_6 = bic(mx6)
) %>% as.data.frame() %>% setNames("BIC") %>% rmarkdown::paged_table()
```

Purely based on the BIC values obtained above we can say that the GMMs with 4 and 5 components are the best due to having the lowest BIC values. However, whilst looking at the density plots from above, one can clearly observe that these model overfit the data as it adapts to local variations along the tails of the GMM. Thus, a natural progression is to observe how we can improve these GMMs through a constrained optimisation where the means of the components are fixed to be the same. Such that we can ignore such local variations.

#### 4.3.2 Constrained EM Optimisation of GMMs

To be able to optimize a GMM where the component means are constrained to be equal, we have to modify the classic expectation maximization algorithm to account for this specific constraint. First off, lets define the basic function for computing the density of a GMM which we will use later.

```{r}
dcomponents <- function(theta, x) {
    mapply(
      function(pi, mu, sigma) pi * dnorm(x, mu, sigma),
      theta$pi, theta$mu, theta$sigma,
      SIMPLIFY = TRUE
    )
}
```

Now that we have the above definition of a GMM density we can define how our new EM algorithm is going to work. As the EM algorithm is broken into two steps (E-step and M-step), we will first define the E-step.

**E-step**

As presented earlier we assume that we have some labels $Z_i$ which indicate what components model what sub-parts of the data. However, this is not something that we can know from beforehand. Thus, we compute the expected value of the statistic $\mathbb{E}[S(Z, Y) | Y ; \theta^{(t-1)}]$. Which we can denote using $\tau_{ik}^t$.

$$
\begin{align}
\tau_{ik}^t &= \mathbb{E}[S(Z,Y) | Y; \theta^{(t-1)}] \\
            &= \dfrac{\pi_k^{(t-1)} f_k(y_i;\theta^{(t-1)})}{\sum_{\mathcal{l}=0}^K \pi_k^{(t-1)} f_{\mathcal{l}} (y_i ; \theta^{(t-1)})}
\end{align}
$$

Indeed, we see that there is no need to change anything in our E-step to achieve this constraint. However, we will have to deal with this in the M-step.

**M-step**

For the M-step we follow a similar procedure but now we have to change the update for the means. Which will take the following form. First we have the same parameters,

$$
\begin{align}
\hat{\pi}_k = \dfrac{\sum_{i=1}^K \tau_{ik}}{n} \hspace{5mm} \mu_k = \dfrac{\sum_{i=1}^K \tau_{ik} x_i}{\sum_{i=1}^K \tau_{ik}} \hspace{5mm} \hat{\sigma}_k^2 = \dfrac{\sum_{i=1}^K \tau_{ik} z_i^2}{\sum_{i=1}^K \tau_{ik}} - \mu_k^2
\end{align}
$$

Note that we do not denote the mean with $\hat{\mu}$ above as it is only used on the optimization for the variance, to compute the new $\hat{\mu}$ we do the following.

$$
\hat{\mu}_k = \frac{\sum_{i=1}^K w_k \mu_k}{\sum_{i=1}^K w_k}
$$

Where $w_k$ is computed by.

$$
w_k = \dfrac{\sum_{i=1}^K \tau_{ik}}{\hat{\sigma}_{k}^2}
$$

We can then use this to create a modified version of the EM algorithm to deterministically obtain components with equal means.

```{r}
# M-step functions, we include the general case such that we can verify the correctness of our implementation.

M_step_general <- function(tau, x) {
  pi    <- colMeans(tau)
  mu    <- colSums(tau * x) / colSums(tau)
  sigma <- sqrt(colSums(tau * x^2) / colSums(tau) - mu^2)
  list(pi = pi, mu = mu, sigma = sigma)
}

M_step_same_means <- function(tau, x) {
  n <- length(x)
  K <- ncol(tau)
  pi    <- colSums(tau) / n
  muk   <- colSums(tau * x) / colSums(tau)
  sigma <- sqrt(colSums(tau * x^2) / colSums(tau) - muk^2)
  wk    <- n * pi / sigma^2
  mu    <- rep(sum(wk * muk) / sum(wk), K)
  list(pi = pi, mu = mu, sigma = sigma)
}
```

```{r}
mixture_gaussian1D <- function(x, theta0, M_step = M_step_general, max_iter = 100, threshold = 1e-6) {

  ## initialization
  n <- length(x)
  likelihoods  <- dcomponents(theta0, x) 
  deviance     <- numeric(max_iter)
  deviance[1]  <- -2 * sum(log(rowSums(likelihoods)))

  for (t in 1:max_iter) {
    
    # E step
    tau <- likelihoods / rowSums(likelihoods)
    # M step
    theta <- M_step(tau, x)
    
    ## Assessing convergence
    likelihoods   <- dcomponents(theta, x)
    deviance[t+1] <- - 2 * sum(log(rowSums(likelihoods)))

    ## prepare next iterations
    
    if (abs(deviance[t + 1] - deviance[t]) < threshold)
      break
    
  }


  list(theta = theta, deviance = deviance[t + 1])
}
```

```{r}
mu_data = mean(y)
pi = c(.5, .5)
means = c(mu_data, mu_data)
variances = c(10,2)

theta0  <- list(pi=pi, mu=means, sigma=variances)
out_general <- mixture_gaussian1D(sp500_data$DailyReturn, theta0, M_step = M_step_same_means)
out_general$theta
```

To then compare the results of using different number of components we additionally define a experiment function.

```{r}
softmax <- function(x) {
  exp_x <- exp(x)
  exp_x / sum(exp_x)
}

constrained_gmm_experiment <- function(y, variances) {
  n_components = length(variances)
  
  mu_data = mean(y)
  
  pi = softmax(rep(1, n_components))
  means = rep(mu_data, n_components)
  
  theta0  <- list(pi=pi, mu=means, sigma=variances)
  out_general <- mixture_gaussian1D(sp500_data$DailyReturn, theta0, M_step = M_step_same_means)
  out_general$theta
  
  # Define the data
  x <- seq(min(y), max(y), length.out = 1000)
  
  # Compute the densities for the two components
  densities <- dcomponents(out_general$theta, x)
  
  # Calculate the final density of the mixture
  final_density <- rowSums(sapply(1:length(out_general$theta$pi), function(i) out_general$theta$pi[i] * dnorm(x, mean = out_general$theta$mu[i], sd = out_general$theta$sigma[i])))
  
  # Plot the data points
  hist(y, breaks = 50, probability = TRUE, col = alpha("skyblue", 0.25), border = alpha("black", 0.25), main = paste("Mixture Model with", n_components, "Components"), xlab = "Daily Return", ylab = "Density")
  
  # Plot the densities for each component
  for (i in 1:n_components) {
    lines(x, densities[,i], col = rainbow(n_components)[i], lwd=2, lty=2)
  }
  
  # Plot the final density of the mixture
  lines(x, final_density, lwd = 2)
  
  # Add legend
  legend_labels <- c(paste0("Component ", 1:n_components), "Mixture")
  legend("topright", legend = legend_labels, col = c(rainbow(n_components), "black"), lty = 1, lwd = 2, pch = 20)
  
  return (out_general=out_general)
}
```

```{r}
mu_mx2 <- constrained_gmm_experiment(y, variances = c(10, 7))
mu_mx3 <-constrained_gmm_experiment(y, variances = c(10, 7, 5))
mu_mx4 <-constrained_gmm_experiment(y, variances = c(10, 7, 5, 3))
mu_mx5 <-constrained_gmm_experiment(y, variances = c(10, 7, 5, 3, 1))
mu_mx6 <-constrained_gmm_experiment(y, variances = c(12, 10, 7, 5, 3, 1))
```

```{r}
mu_mx2
```

By simply observing the produced densities for the new GMMs it is much better then the standard GMM approach. As it quite nicely ignores such local variantions along the tails of the distribution. To evaluate the different fits for these Gaussians we perform the BIC tests again.

```{r}
bic_devicances <- function(model) {
  n_free_params <- length(out_general$theta$pi)
  n <- length(y)
  
  return(model$deviance + log(n) * n_free_params)
}

rbind(
  gmm_2 = bic_devicances(mu_mx2),
  gmm_3 = bic_devicances(mu_mx3),
  gmm_4 = bic_devicances(mu_mx4),
  gmm_5 = bic_devicances(mu_mx5),
  gmm_6 = bic_devicances(mu_mx6)
) %>% as.data.frame() %>% setNames("BIC (Constrained GMM)") %>% rmarkdown::paged_table()

rbind(
  gmm_2 = bic(mx2),
  gmm_3 = bic(mx3),
  gmm_4 = bic(mx4),
  gmm_5 = bic(mx5),
  gmm_6 = bic(mx6)
) %>% as.data.frame() %>% setNames("BIC (Standard GMM)") %>% rmarkdown::paged_table()
```

We already see a significant improvement in BIC for the constrained optimization version of the GMM, here the 5 component model is the one with the lowest BIC value and thus the best model in terms of BIC. Therefore, this demonstrates clearly the fact that the standard GMM was overfitting to the data and was not able to converge to the optimal solution.

### 4.4 Student t-distribution

**Task description:** As a third and last model, we propose to model the daily return as coming from a location-scale family of Student distributions, which is a model defined by the collection of densities $$f_{\nu,m,a}(x) = \frac{\Gamma\big(\frac{\nu+1}{2}\big)}{\sqrt{\pi \nu a^2}\Gamma\big(\frac{\nu}{2}\big)}\Bigg(1+\frac{(x-m)^2}{\nu a^2} \Bigg)^{-\frac{\nu+1}{2}}$$ where $\Gamma$ is the Gamma function and $\theta=(\nu,m,a) \in \mathbb{R}_+^*\times \mathbb{R}\times \mathbb{R}_+^*$ is the parameter.

#### 4.4.1 Motivation behind model selection

**Task description:** What is the motivation for considering this model?

Indeed from looking at the above examples for the standard GMM the final fits are still not great even when we add additional components to account for the variable variance of the distribution. But as we observe the distribution it is more clear that it can follow more closely a student t-distribution given its long tails and elongated middle density.

#### 4.4.2 Fitting the student t-distribution

**Task description:** Propose an algorithm for estimating the parameter of this model and use it to fit this model to the data.

Given that we have the PDF of the student t-distribution above, we want to minimise its negative log-likelihood over the data. Which will result in of course maximising the overall likelihood and thus obtain a convergence. One can achieve this through an optimiser such as L-BFGS-B which is a gradient based method which computes the gradient $\nabla f(x; \theta)$ where in our case the loss function is defined by the negative log likelihood. We perform negative log likelihood due to the fact that this method aims to minimise along the largest negative gradient, and we use log values to avoid gradient vanishing through large float values.

```{r}
# Define log-likelihood function for Student's t-distribution
neglog_likelihood_student_t <- function(theta, x) {
  nu <- theta[1]
  mu <- theta[2]
  sigma <- theta[3]
  
  # Check if sigma is positive
  if(sigma <= 0) {
    return(Inf)
  }
  
  # Check if nu is at least 1
  if(nu < 1) {
    nu <- 1
  }
  
  loglik <- sum(dt((x - mu) / sigma, df = nu, log = TRUE)) - length(x) * log(sigma)
  
  return(-loglik)  # Negative log-likelihood for minimization
}

# Fit Student's t-distribution model using MLE
fit_student_t <- function(y) {
  if (any(!is.finite(y))) {
    stop("Input data contains non-finite values.")
  }
  
  y <- y[is.finite(y)]
  
  if (length(y) < 3) {
    stop("Insufficient data points for estimation.")
  }
  
  init_params <- c(5, mean(y), sd(y))
  opt_result <- optim(init_params, neglog_likelihood_student_t, x = y, method = "L-BFGS-B",
                      lower = c(0, -Inf, 0), upper = c(Inf, Inf, Inf))
  
  # Extract optimized parameters
  nu <- opt_result$par[1]
  mu <- opt_result$par[2]
  sigma <- opt_result$par[3]
  
  # Return parameter estimates
  return(list(nu = nu, mu = mu, sigma = sigma))
}

# Fit Student's t-distribution model to the data
student_t_params <- fit_student_t(y)
student_t_params
```

```{r}
# Generate data for the PDF using the fitted parameters
x_range <- seq(min(y), max(y), length.out = 1000)
pdf_values <- dt((x_range - student_t_params$mu) / student_t_params$sigma, 
                 df = student_t_params$nu) / student_t_params$sigma

# Create a data frame for plotting
plot_data <- data.frame(x = x_range, pdf = pdf_values)

# Plot histogram of the data and fitted PDF
ggplot() +
  geom_histogram(data = data.frame(y), aes(x = y, y = after_stat(density)), bins = 30, fill = "skyblue", color = "black") +
  geom_line(data = plot_data, aes(x = x, y = pdf), color = "red", size = 1) +
  labs(title = "Fitted Student's t-distribution Model",
       x = "Daily Return",
       y = "Density") +
  theme_minimal()
```

As we can observe we obtain a very nice fit for the distribution of the data using the student t-distribution, demonstrating the fact that the data seems to more distributed along a student t-distribution. We will in now compare how this model performs wrt the other methods in the following section.

### 4.5 Model evaluation

**Task description:** Between all the previous models (the normal, the 5 mixtures of normals, and the location-scale Student) which one do you choose? Explain your methodology.

To effectively evaluate the models we can employ a few techniques, the most obvious is to apply the BIC metric to measure how well the models capture the distribution of the data. Which is simply done by applying the following formula:

$$
BIC = k \hspace{1mm} ln(n) - 2 \hspace{1mm} ln(\hat{L})
$$

Where we have the following parameters to compute:

-   $\hat{L}$: the maximized likelihood of the model.

-   $n$: the number of data points in the dataset.

-   $k$: the number of free parameters in our model (note for the constrained GMM this will be different).

We then go ahead and apply this metric to the models so far and put the results in a table seen below.

```{r}
bic_benchmark <- function(x, out_model, n_free_params) {
  n <- length(x)
  bic_values = list()
  
  return(out_model$deviance + log(n) * n_free_params)
}
```

```{r}
mu_data = mean(y)
pi = c(.2, .2, .2, .2, .2)
means = c(mu_data, mu_data, mu_data, mu_data, mu_data)
variances = c(10, 7, 5, 3, 1)

theta0  <- list(pi=pi, mu=means, sigma=variances)
out_general <- mixture_gaussian1D(sp500_data$DailyReturn, theta0, M_step = M_step_general)

b_5unconstrained <- bic_benchmark(y, out_general, 3 * 5)

out_general_constrained <- mixture_gaussian1D(sp500_data$DailyReturn, theta0, M_step = M_step_same_means)

b_5constrained <- bic_benchmark(y, out_general_constrained, 2 * 5 + 1) # Only one mu is optimised

theta = student_t_params
b_tdist = log(length(y)) + 2 * neglog_likelihood_student_t(c(theta$nu, theta$mu, theta$sigma), y)

single_guass <- fitdistr(sp500_data$DailyReturn, "normal")
b_single_gauss <- BIC(single_guass)
```

```{r}
rbind(
  five_comp_gmm             = b_5unconstrained,
  five_comp_gmm_constrained = b_5constrained,
  student_tdistribution     = b_tdist,
  single_gaussian           = b_single_gauss
) %>% as.data.frame() %>% setNames("BIC") %>% rmarkdown::paged_table()
```

By only observing the BIC values it is clear that the 5 component GMM constrained on means is the best model. Notably we might want to reject the single Gaussian model completely, and we can do this through the Kolmogorov-Smirnov test can work for this. Where we have the following null- and alternative hypothesis are defined as follows.

$$
H_0: \text{the empirical distribution of the data fits the theoretical distribution} \\
H_1: \text{The empirical distribution does not fit the theoretical distribution}
$$

```{r}
ks.test(y, "pmixture", theta0)
```

Thus, we can quite confidently conclude that we can reject the single Gaussian as a model (as the null-hypothesis is rejected) and announce the 5 component constrained GMM as the "best" model. However, this does not mean that the student t-distribution doesn't model the data well.

### 4.6 Expected daily return

**Task description:** Is the expected daily return different than zero?

As we have seen above, a single normal cannot model the data effectively. Thus, one will have to use an hypothesis with no normality assumption. Which can be done through a Wilcoxon test as we have seen in the previous exercises.

```{r}
wilcox.test(y, mu=0, alternative="two.sided")
```

Based on the results of this test, we can reject the null-hypothesis and thus conclude that the expected mean is not equal to 0. Which further highlights the fact that the student t-distribution is not the best model for this data given the fact that the mean of a student t-distribution is 0.
