---
title: "MAP566 - Homework assignment #1"
format:
  html:
    self-contained: true
    theme: [cosmo, theme.scss]
    toc: true
    number-sections: true
    html-math-method: katex
    code-copy: true
    code-summary: "Show the code"
    code-overflow: wrap
---

# Single comparison

```{r,warning=FALSE}
library(readr)
library(dplyr)
library(tidyverse)
library(MASS)
library(mixtools)

NHANES <- read_csv("NHANES_age_prediction 3.csv")
data = NHANES[,c("DIQ010","age_group","RIDAGEYR","RIAGENDR","PAQ605","BMXBMI","LBXGLU","LBXGLT","LBXIN")]
colnames(data) = c("Diabete","age_group","Age","Sex","Phys_activ","BMI","Glu","Glu2h","BIL")
```

We study data from the National Health and Nutrition Examination Survey (NHANES), administered by the Centers for Disease Control and Prevention (CDC), which collects extensive health and nutritional information from a diverse U.S. population.

```{r}
rmarkdown::paged_table(data)
```

The variables under study are :

-   `Diabete`: diabete diagnosis (1: Yes, 2: No, 3: Borderline);
-   `age_group` : age group of respondent ("Adult" or "Senior");
-   `Age` : the age of respondent;
-   `Sex`: sex of respondent (1: Male, 2: Female);
-   `Physical activity`: respondent's answer to the question "Does your work involve vigorous-intensity activity that causes large increases in breathing or heart rate like carrying or lifting heavy loads, digging or construction work for at least 10 minutes continuously? (1: Yes, 2: No, 7: no answer);
-   `BMI` : Body Mass Index if respondent ($kg/m^{2}$);
-   `Glu` : Respondent's Blood Glucose after fasting ($mg/dL$);
-   `Glu2h` : Respondent's Two Hours Blood Glucose ($mg/dL$);
-   `BIL` : Respondent's Blood Insulin Level ($pmol/L$).

1.  Test if the mean level of blood glucose after fasting is the same for diabetic and non diabetic respondents (*hint:* plot first the data and justify the test(s) to use).

    To investigate if the mean level of blood glucose after fasting is the same for diabetic and non-diabetic we should first split the data and plot the distributions side by side to get an idea for which test will be the most appropriate.

    ```{r}
    diabetic <- data %>% filter(Diabete == 1)
    non_diabetic <- data %>% filter(Diabete == 2)

    rmarkdown::paged_table(diabetic)
    rmarkdown::paged_table(non_diabetic)
    ```

    ```{r}
    diabetic$Group <- "Diabetic"
    non_diabetic$Group <- "Non-Diabetic"

    combined_data <- rbind(diabetic, non_diabetic)

    ggplot(combined_data, aes(x = Group, y = Glu)) +
      geom_boxplot() +
      labs(x = "Status", y = "Glucose") +
      ggtitle("Glucose Distribution by Diabetes Status") +
      theme_minimal()

    ggplot(combined_data, aes(x = Group, y = Glu, fill = Group)) +
      geom_violin(trim = FALSE) +
      labs(x = "Status", y = "Glucose") +
      ggtitle("Glucose Distribution by Diabetes Status") +
      theme_minimal()
    ```

    It is clear that the glucose distribution after fasting for the two groups have vastly different variances, thus a two-sample t-test would not be applicable due to its equal variance assumption. Further, it appears that the two distributions follow somewhat the normal assumption. **However, we do observe that the diabetic distribution has an elongated tail towards higher glucose levels which could be breaking this assumption.**

    [**TODO :: Double check that indeed the diabetic distribution follows the normal assumption**]{.underline}

    ```{r}
    var.test(diabetic$Glu, non_diabetic$Glu, var.equal=TRUE)
    ```

    Further, it is now clear that indeed the variances are not the same for the two distributions given that we can clearly reject the null hypothesis stating that the two variances are the same.

    We will thus be using a Welch two sample t-test to examine whether or not the means of the two groups are the same given that this test does not assume equal variance.

    [**TODO :: Add the setup for the null hypothesis**]{.underline}

    ```{r}
    t.test(diabetic$Glu, non_diabetic$Glu)
    ```

    [**TODO :: Write conclusion about the results observed**]{.underline}

2.  Test for the diabetic respondents if the mean level of blood glucose after fasting is the same for adults and seniors.

    Again we need to investigate the distribution for the two groups to understand which assumptions for the different hypothesis tests will be applicable.

    ```{r}
    diabetic_adult <- data %>% filter(Diabete == 1 & age_group == "Adult")
    diabetic_seniors <- data %>% filter(Diabete == 1 & age_group == "Senior")

    rmarkdown::paged_table(diabetic_adult)
    rmarkdown::paged_table(diabetic_seniors)
    ```

    ```{r}
    diabetic_adult$Group <- "Diabetic Adult"
    diabetic_seniors$Group <- "Diabetic Senior"

    combined_data <- rbind(diabetic_adult, diabetic_seniors)

    ggplot(combined_data, aes(x = Group, y = Glu)) +
      geom_boxplot() +
      labs(x = "Age Group", y = "Glucose") +
      ggtitle("Glucose Distribution for Diabetics by Age Group") +
      theme_minimal()

    ggplot(combined_data, aes(x = Group, y = Glu, fill = Group)) +
      geom_violin(trim = FALSE) +
      labs(x = "Age Group", y = "Glucose") +
      ggtitle("Glucose Distribution for Diabetics by Age Group") +
      theme_minimal()
    ```

    From this we can observe that the distribution for the diabetic adults and diabetic seniors do not have the same variance and that they do not follow the normal assumption. Which is clear by the fact that for seniors we have two distinct peaks, and for adults we have a trailing tail for higher glucose levels which indicate that they do not follow the parametric Gaussian distribution. Making the two sample Welch t-test not applicable in this scenario. Thus, to circumvent these limiting assumptions in this case we will be performing a Wilcoxon test which does not have an underlying parametric assumption about the distribution of the data.

    [**TODO :: Add the maths behind to frame the problem first**]{.underline}

    ```{r}
    wilcox.test(diabetic_adult$Glu, diabetic_seniors$Glu, alternative="two.sided")
    ```

    [**TODO :: Add conclusion about the results of the test**]{.underline}

3.  Is it possible to test for the adults diabetic respondents if the mean level of blood glucose after fasting is the same for those who have a vigorous work activity and for those who have not?

    [**TODO :: Write motivation for the test**]{.underline}
    TODO :: Apply power of t-test such that we can examine if we can test this
```{r}
diabetic_active <- data %>% filter(Diabete == 1 & age_group == "Adult" & Phys_activ == 1)
diabetic_inactive <- data %>% filter(Diabete == 1 & age_group == "Adult" & Phys_activ == 2)

diabetic_active$Group <- "Diabetic Active"
diabetic_inactive$Group <- "Diabetic Inactive"

# Combine the data frames
combined_data <- rbind(diabetic_active, diabetic_inactive)
combined_data
```

    ```{r}
    diabetic_active <- data %>% filter(Diabete == 1 & age_group == "Adult" & Phys_activ == 1)
    diabetic_inactive <- data %>% filter(Diabete == 1 & age_group == "Adult" & Phys_activ == 2)

    diabetic_active$Group <- "Diabetic Active"
    diabetic_inactive$Group <- "Diabetic Inactive"

    # Combine the data frames
    combined_data <- rbind(diabetic_active, diabetic_inactive)

    # Create boxplot
    ggplot(combined_data, aes(x = Group, y = Glu)) +
      geom_boxplot() +
      labs(x = "Work Activity", y = "Glucose") +
      ggtitle("Glucose Distribution for Diabetic Adults by Work Activity") +
      theme_minimal()

    # Create violin plot
    ggplot(combined_data, aes(x = Group, y = Glu, fill = Group)) +
      geom_violin(trim = FALSE) +
      labs(x = "Work Activity", y = "Glucose") +
      ggtitle("Glucose Distribution for Diabetic Adults by Work Activity") +
      theme_minimal()
    ```

    ```{r}
    wilcox.test(diabetic_active$Glu, diabetic_inactive$Glu, alternative="two.sided")
    ```

    [**TODO :: Add conclusion and analysis about the results**]{.underline}

4.  Test if the proportion of diabetic is the same for male and female respondents. Compare conclusions of several tests. You can use the function `table` to compute contingency tables.

TODO :: Compare several tests

    ```{r}
    # Create a contingency table
    contingency_table <- table(data$Diabete, data$Sex)
    rownames(contingency_table) <- c("No Diabetes", "Diabetes", "Borderline")
    colnames(contingency_table) <- c("Male", "Female")

    # Display the contingency table
    contingency_table

    # Perform chi-square test
    chi_square_test <- chisq.test(contingency_table)

    # Display the results
    chi_square_test
    ```

# Gene expression data

The *liver* dataset contains measurements of rat liver toxicity levels (measured through cholesterol levels) as well as measurements of the expression levels of several thousand genes.

When loading the data (file `liver_data.rda`), the table *liver* is created with 64 rows (the observations) and 3117 columns. The data can be loaded using the following code:

```{r}
load("liver_data.rda")
```

The first column, `cholesterol`, is the variable to be explained. The remaining 3116 are the expressions of 3116 genes (more precisely, the logarithm of the ratio between expression levels in two experimental conditions). The aim is to identify the variables (and therefore the genes) linked to the response.

1.  We first want to study the gene which is the most correlated with the level of cholesterol. Identify this gene and plot `cholesterol` as a function of this gene expression level. Fit a linear model. Test if there is a significant relation between this gene expression and `cholesterol`?

2.  We now wish to perform the previous test for all of the 3116 genes. For each of the genes, fit a linear model that explains `cholesterol` as a function of the gene expression and compute the $p$-value of the test.

3.  Order the $p$-values and plot the ordered $p$-values as a function of their rank. On the same plot, display the line $y = x/3116$. Discuss the result.

4.  Identify a set of genes linked to the response (*aka* discoveries). We want to guarantee that the expected proportion of false discoveries (mistakes) is less than $5\%$. Explain how you proceed and how many genes you discover.

5.  We wish to be more conservative and guarantee that the probability of making a false discovery (or more) is less than $5\%$. Explain how you proceed and how many genes you discover.

# Non parametric regression

1.  Upload the dataset contained in the file `data_exo3.csv` and plot the data.

2.  Try several polynomial models for the data, select a model and comment the results.

3.  Fit a nonlinear model and compare with the polynomial model you have selected.

4.  Calculate confidence intervals for $\mathbb{E}(Y_{new})$ and prediction intervals for $Y_{new}$ on the following values of the covariate: $$
    x_{new} = (1,...,10). 
    $$

# S&P500 daily return

The Standard and Poor's 500 or simply the S&P 500, is a stock market index tracking the performance of 500 large companies listed on stock exchanges in the United States. It is one of the most commonly followed equity indices.

The dataset `sp500_history.csv` contains information about the value of the index at the opening and closing of the market, every day from January 3rd, 2007 to February 2nd, 2024. The goal of the study is to understand the distribution of the *Daily Return* during this period. The daily return is defined as the index difference between closing and opening.

1.  Load the dataset `sp500_history.csv` into a *data frame* and add a column that computes the daily return for each days.

    ```{r}
    sp500_data <- read_csv("sp500_history.csv")
    sp500_data$DailyReturn <- sp500_data$Close - sp500_data$Open

    # Check for non-numeric values in DailyReturn column
    non_numeric_count <- sum(is.na(sp500_data$DailyReturn) | !is.numeric(sp500_data$DailyReturn))

    # Print the count of non-numeric values
    cat("Number of non-numeric values in DailyReturn column:", non_numeric_count, "\n")

    # If there are non-numeric values, print their indices
    if (non_numeric_count > 0) {
      cat("Indices of non-numeric values in DailyReturn column:", which(!is.numeric(sp500_data$DailyReturn)), "\n")
    } else {
      cat("No non-numeric values found in DailyReturn column.\n")
    }

    rmarkdown::paged_table(sp500_data)
    ```

2.  We propose to model the daily return as a sample from a normal population. Write the model and use *R* to fit it to the data. What do you think of this model?

    ```{r}
    ggplot(sp500_data, aes(x = Date, y = DailyReturn)) +
      geom_histogram(stat = "identity", fill = "grey", color = "grey", bins = 15) +
      labs(title = "Daily Return of S&P 500 (2007-2010)",
           x = "Date",
           y = "Daily Return") +
      theme_minimal() +
      theme(axis.text.x = element_blank(),
            axis.title.x = element_blank())
    ```

    Under Louis Bachelier's "Theory of Speculation" assume that the returns can be sampled from a perfectly random distribution, in this case this would be a normal distribution which we can parametirise through a Gaussian model. Assume that the return is modeled by some random variable $X_r$ and that some value of that RV is denoted by $x_t$. Thus, since we first assume Bachelier's theory we can model the RV through the following Gaussian.

    $$
    X_r \sim \mathcal{N}(\mu_r, \sigma_r^2)
    $$

    Where the model parameters are the empirical mean and empirical variance expressed by:

    $$
    \mu_r = \dfrac{\sum_{i=0}^N x_i}{N} \hspace{5mm} \sigma_r^2 = \dfrac{\sum_{i=0}^N (x_i - \mu_r)}{N}
    $$

    We can then directly fit such a Gaussian model in R through the following:

    ```{r}
    fit <- fitdistr(sp500_data$DailyReturn, "normal")

    # Print the fitted parameters
    print(fit)

    # Plot the histogram of daily returns with the fitted Gaussian distribution
    hist(sp500_data$DailyReturn, breaks = 100, freq = FALSE, 
         main = "Histogram of Daily Returns with Fitted Gaussian Distribution",
         xlab = "Daily Return")
    curve(dnorm(x, mean = fit$estimate[1], sd = fit$estimate[2]), 
          col = "blue", lwd = 2, add = TRUE, yaxt = "n")
    ```

    Indeed, the it seems that Bachelier's theory does describe the price movements well. As the daily return resembles a normal distribution, however, we do observe that that tails seem to follow a higher variance compared to the density around the mean which seems to have a smaller variance. Which also explains why our initial Gaussian doesn't effectively capture this density.

    **TODO :: We should maybe fix the means and use two components as we see the bottom part has a larger variance compared to the middle parts.**

    **TODO :: Compare the convergence graphs for the naive approach with no optimisation constraints with the one where we use a optimisation constrain on the means**

3.  Instead of a single normal, we now propose to use a mixture of normals with $p$ components. Write the corresponding models and use *R* to fit them to the data for all values of $p$ between $2$ and $6$. Discuss the results and compare them with the model of Question 1.

    To combine the multiple Gaussians to model a single valid PDF, we will have to introduce a weight parameter which will be used between the weighted sum of Gaussian components. First, we denote a single component the following way.

    $$
    Y_r \sim \mathcal{N}(\theta_k) \hspace{5mm} \text{Where $\theta$ is a parameter vector for component $k$ defined by} \hspace{5mm} \theta_k = \left[ \mu_k, \sigma_k^2 \right]
    $$

    Furthermore, to form the mixture we introduce the weight parameter $\pi_k$ which is probabilistic and forms the weighted sum (GMM).

    $$
    Y_i = \sum_{k=1}^K \pi_k \mathcal{N}(\theta_k)
    $$

    As this is a weighted sum and we expect $Y_i$ to be modeled from a valid PDF, we have to constrain the weights such that:

    $$
    \pi_k \geq 0 \hspace{2.5mm} \land \hspace{2.5mm} \sum_{k=1}^K \pi_k = 1
    $$

    Which is the case as long as we follow that $\pi_k$ comes from a PDF defined over the categorical RVs $Z_i$ which defines the different components such that we have:

    $$
    P(Z_i = k) = \pi_k \hspace{5mm} \text{Where $Z$ is a sequence of RVs} \hspace{5mm} \{1, 2, \dots, K\}
    $$

    Thus, we observe that we first have to initialize the parameters of the model which we can do by placing the means based on a K-Means initialization and set some explicit variance which will serve as the initialization for the iterative EM-algorithm for parameter optimization over such a GMM.

    First off, we will naively apply a GMM routine to this problem using expectation maximization which we will compare against an constrained optimization approach where we ensure equal means for all distributions as it seems to only be the variance that's needed to change based on our earlier graph.

    ## Unconstrained GMM Optimization

    ```{r}
    # First we apply K-Means clustering on the graph to optain initial means
    n_components = 3
    y = sp500_data$DailyReturn

    kmeans_out <- kmeans(y, centers = n_components)
    mu = as.vector(kmeans_out$centers)
    mu
    ```

    This results seems strange given that we should want Gaussian components around the same mean. However, for this experiment we will naively apply k-Means initialization without any constraints in the optimization and observe what results we obtain.

    ```{r}
    fit_mixture_model <- function(n_components, y, max_iter = 300) {
      kmeans_out <- kmeans(y, centers = n_components)
      mu <- as.vector(kmeans_out$centers)
      mixture_EM <- normalmixEM(y, mu = mu, k = n_components, arbvar = FALSE, maxit = max_iter)
      
      log_likelihood <- mixture_EM$loglik
      print(log_likelihood)
      result <- list(pi = mixture_EM$lambda, mu = mixture_EM$mu, sigma = mixture_EM$sigma)
      
      result <- list(pi = mixture_EM$lambda, mu = mixture_EM$mu, sigma = mixture_EM$sigma)
      
      # Plot histogram with 50 bars
      hist(y, breaks = 50, probability = TRUE, col = alpha("skyblue", 0.25), border = alpha("black", 0.25), main = paste("Mixture Model with", n_components, "Components"), xlab = "Daily Return", ylab = "Density")
      
      # Add density line
      lines(density(mixture_EM$x), col="red", lwd=2)
      
      # Plot densities of Gaussian components
      x <- seq(min(y), max(y), length.out = 1000)
      for (i in 1:n_components) {
        curve(result$pi[i] * dnorm(x, mean = result$mu[i], sd = result$sigma[i]), add = TRUE, col = i + 1, lwd=2, lty=2)
      }
      
      return(list(result = result, mixture_EM = mixture_EM, log_likelihood = log_likelihood))
    }


    ```

    ```{r}
    mx2 <- fit_mixture_model(2, y)
    mx3 <- fit_mixture_model(3, y)
    mx4 <- fit_mixture_model(4, y)
    mx5 <- fit_mixture_model(5, y)
    mx6 <- fit_mixture_model(6, y)

    mixtures = c(mx2, mx3, mx4, mx5, mx6)
    ```

    We can observe that the final fits are indeed much better than the single Gaussian example. However, we do observe that there is some overfitting happening at the tails of the distribution for all of the experiments we have run. This is caused by the fact that the means are not the same for all the Gaussian components and thus for our next section we will be looking at constrained optimization of this GMM where the means should be the same.

## Constrained GMM Optimisation

```{r}
dcomponents <- function(theta, x) {
    mapply(
      function(pi, mu, sigma) pi * dnorm(x, mu, sigma),
      theta$pi, theta$mu, theta$sigma,
      SIMPLIFY = TRUE
    )
}
```

```{r}

M_step_general <- function(tau, x) {
  pi    <- colMeans(tau)
  mu    <- colSums(tau * x) / colSums(tau)
  sigma <- sqrt(colSums(tau * x^2) / colSums(tau) - mu^2)
  list(pi = pi, mu = mu, sigma = sigma)
}

M_step_same_means <- function(tau, x) {
  n <- length(x)
  K <- ncol(tau)
  pi    <- colSums(tau) / n
  muk   <- colSums(tau * x) / colSums(tau)
  sigma <- sqrt(colSums(tau * x^2) / colSums(tau) - muk^2)
  wk    <- n * pi / sigma^2
  mu    <- rep(sum(wk * muk) / sum(wk), K)
  list(pi = pi, mu = mu, sigma = sigma)
}

```

```{r}

mixture_gaussian1D <- function(x, theta0, M_step = M_step_general, max_iter = 100, threshold = 1e-6) {

  ## initialization
  n <- length(x)
  likelihoods  <- dcomponents(theta0, x) 
  deviance     <- numeric(max_iter)
  deviance[1]  <- -2 * sum(log(rowSums(likelihoods)))

  for (t in 1:max_iter) {
    
    # E step
    tau <- likelihoods / rowSums(likelihoods)
    # M step
    theta <- M_step(tau, x)
    
    ## Assessing convergence
    likelihoods   <- dcomponents(theta, x)
    deviance[t+1] <- - 2 * sum(log(rowSums(likelihoods)))

    ## prepare next iterations
    
    if (abs(deviance[t + 1] - deviance[t]) < threshold)
      break
    
  }


  list(theta = theta, deviance = deviance[t + 1])
}

```

```{r}
mu_data = mean(y)
pi = c(.5, .5)
means = c(mu_data, mu_data)
variances = c(10,2)

theta0  <- list(pi=pi, mu=means, sigma=variances)
out_general <- mixture_gaussian1D(sp500_data$DailyReturn, theta0, M_step = M_step_same_means)
out_general$theta
```

```{r}
softmax <- function(x) {
  exp_x <- exp(x)
  exp_x / sum(exp_x)
}

constrained_gmm_experiment <- function(y, variances) {
  n_components = length(variances)
  
  mu_data = mean(y)
  
  pi = softmax(rep(1, n_components))
  means = rep(mu_data, n_components)
  
  theta0  <- list(pi=pi, mu=means, sigma=variances)
  out_general <- mixture_gaussian1D(sp500_data$DailyReturn, theta0, M_step = M_step_same_means)
  out_general$theta
  
  # Define the data
  x <- seq(-150, 150, length.out = 1000)
  
  # Compute the densities for the two components
  densities <- dcomponents(out_general$theta, x)
  
  # Calculate the final density of the mixture
  final_density <- rowSums(sapply(1:length(out_general$theta$pi), function(i) out_general$theta$pi[i] * dnorm(x, mean = out_general$theta$mu[i], sd = out_general$theta$sigma[i])))
  
  # Plot the data points
  hist(y, breaks = 50, probability = TRUE, col = alpha("skyblue", 0.25), border = alpha("black", 0.25), main = paste("Mixture Model with", n_components, "Components"), xlab = "Daily Return", ylab = "Density")
  
  # Plot the densities for each component
  for (i in 1:n_components) {
    lines(x, densities[,i], col = rainbow(n_components)[i], lwd=2, lty=2)
  }
  
  # Plot the final density of the mixture
  lines(x, final_density, lwd = 2)
  
  # Add legend
  legend_labels <- c(paste0("Component ", 1:n_components), "Mixture")
  legend("topright", legend = legend_labels, col = c(rainbow(n_components), "black"), lty = 1, lwd = 2, pch = 20)
}
```

```{r}
constrained_gmm_experiment(y, variances = c(10, 5))
```

```         
**TODO :: Compare each using BIC**
```

4.  As a third and last model, we propose to model the daily return as coming from a location-scale family of Student distributions, which is a model defined by the collection of densities $$f_{\nu,m,a}(x) = \frac{\Gamma\big(\frac{\nu+1}{2}\big)}{\sqrt{\pi \nu a^2}\Gamma\big(\frac{\nu}{2}\big)}\Bigg(1+\frac{(x-m)^2}{\nu a^2} \Bigg)^{-\frac{\nu+1}{2}}$$ where $\Gamma$ is the Gamma function and $\theta=(\nu,m,a) \in \mathbb{R}_+^*\times \mathbb{R}\times \mathbb{R}_+^*$ is the parameter.

    1.  What is the motivation for considering this model?

        Indeed from looking at the above examples the final fits are still not great even when we add additional components to account for the variable variance of the distribution. But as we observe the distribution it is more clear that it can follow more closely a student t-distribution given its long tails and elongated middle density.

    2.  Propose an algorithm for estimating the parameter of this model and use it to fit this model to the data.

```{r}
# Define log-likelihood function for Student's t-distribution
log_likelihood_student_t <- function(theta, x) {
  nu <- theta[1]
  mu <- theta[2]
  sigma <- theta[3]
  
  # Check if sigma is positive
  if(sigma <= 0) {
    return(Inf)
  }
  
  # Check if nu is at least 1
  if(nu < 1) {
    nu <- 1
  }
  
  loglik <- sum(dt((x - mu) / sigma, df = nu, log = TRUE)) - length(x) * log(sigma)
  
  return(-loglik)  # Negative log-likelihood for minimization
}

# Fit Student's t-distribution model using MLE
fit_student_t <- function(y) {
  # Check for non-finite values in the data
  if (any(!is.finite(y))) {
    stop("Input data contains non-finite values.")
  }
  
  # Remove missing values
  y <- y[is.finite(y)]
  
  # Check if there are enough data points
  if (length(y) < 3) {
    stop("Insufficient data points for estimation.")
  }
  
  # Initial parameter guess
  init_params <- c(5, mean(y), sd(y))
  
  # Optimize log-likelihood function
  opt_result <- optim(init_params, log_likelihood_student_t, x = y, method = "L-BFGS-B",
                      lower = c(0, -Inf, 0), upper = c(Inf, Inf, Inf))
  
  # Extract optimized parameters
  nu <- opt_result$par[1]
  mu <- opt_result$par[2]
  sigma <- opt_result$par[3]
  
  # Return parameter estimates
  return(list(nu = nu, mu = mu, sigma = sigma))
}

# Fit Student's t-distribution model to the data
student_t_params <- fit_student_t(y)
student_t_params
```

```{r}
# Generate data for the PDF using the fitted parameters
x_range <- seq(min(y), max(y), length.out = 1000)
pdf_values <- dt((x_range - student_t_params$mu) / student_t_params$sigma, 
                 df = student_t_params$nu) / student_t_params$sigma

# Create a data frame for plotting
plot_data <- data.frame(x = x_range, pdf = pdf_values)

# Plot histogram of the data and fitted PDF
ggplot() +
  geom_histogram(data = data.frame(y), aes(x = y, y = ..density..), bins = 30, fill = "skyblue", color = "black") +
  geom_line(data = plot_data, aes(x = x, y = pdf), color = "red", size = 1) +
  labs(title = "Fitted Student's t-distribution Model",
       x = "Daily Return",
       y = "Density") +
  theme_minimal()
```

5.  Between all the previous models (the normal, the 5 mixtures of normals, and the location-scale Student) which one do you choose? Explain your methodology.

6.  Is the expected daily return different than zero?
